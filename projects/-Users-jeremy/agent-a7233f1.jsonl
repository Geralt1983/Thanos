{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","type":"user","message":{"role":"user","content":"Explore the LLM Council codebase to understand why the council might be giving generic/boring/useless responses. Focus on:\n\n1. The prompts used in each stage (Stage 1, Stage 2, Stage 3)\n2. The system prompts or instructions given to models\n3. How context is passed between stages\n4. The chairman synthesis prompt\n5. Any configuration that affects response quality\n\nLook in backend/council.py and any related files. Report on what you find regarding prompt design and any issues that could lead to generic responses."},"uuid":"16e42f91-3151-4b43-86e8-4534b1fcefcb","timestamp":"2026-01-02T02:30:42.338Z"}
{"parentUuid":"16e42f91-3151-4b43-86e8-4534b1fcefcb","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01L3QNGsjTPBniTQbUkktXoQ","type":"message","role":"assistant","content":[{"type":"text","text":"I'll explore the LLM Council codebase to understand the prompt design and identify issues that could lead to generic responses. Let me start by understanding the project structure."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":66644,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":66644,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWhmMfuJGZ3a1wqzGVVbu","type":"assistant","uuid":"ba29696f-3bcb-4fc5-a70d-993ceecb4dc7","timestamp":"2026-01-02T02:30:47.416Z"}
{"parentUuid":"ba29696f-3bcb-4fc5-a70d-993ceecb4dc7","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01L3QNGsjTPBniTQbUkktXoQ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01FtjeJrqfehBR9ihcH72X2m","name":"Glob","input":{"pattern":"**/*.py","path":"/Users/jeremy/Projects/LLMCouncil/frontend"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":66644,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":66644,"ephemeral_1h_input_tokens":0},"output_tokens":121,"service_tier":"standard"}},"requestId":"req_011CWhmMfuJGZ3a1wqzGVVbu","type":"assistant","uuid":"ac7ad656-5d33-4a37-8732-49d923ebcebb","timestamp":"2026-01-02T02:30:47.913Z"}
{"parentUuid":"ac7ad656-5d33-4a37-8732-49d923ebcebb","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FtjeJrqfehBR9ihcH72X2m","type":"tool_result","content":"/Users/jeremy/Projects/LLMCouncil/frontend/node_modules/flatted/python/flatted.py"}]},"uuid":"4fbf9af3-e6dd-444f-a461-e875ce70a78c","timestamp":"2026-01-02T02:30:52.460Z","toolUseResult":{"filenames":["/Users/jeremy/Projects/LLMCouncil/frontend/node_modules/flatted/python/flatted.py"],"durationMs":4150,"numFiles":1,"truncated":false}}
{"parentUuid":"4fbf9af3-e6dd-444f-a461-e875ce70a78c","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_017i65uhSkEcahz9zWC6iMSY","type":"message","role":"assistant","content":[{"type":"text","text":"Let me look for the backend directory:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":194,"cache_read_input_tokens":66644,"cache_creation":{"ephemeral_5m_input_tokens":194,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWhmNPaJxaM5U1731NMwE","type":"assistant","uuid":"09da38e1-bf85-47af-bb9c-bd347f1f3485","timestamp":"2026-01-02T02:30:55.921Z"}
{"parentUuid":"09da38e1-bf85-47af-bb9c-bd347f1f3485","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_017i65uhSkEcahz9zWC6iMSY","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01M6rJSeREzXWmRwpfe1fGSw","name":"Bash","input":{"command":"find /Users/jeremy/Projects/LLMCouncil -name \"council.py\" -o -name \"*.py\" -path \"*/backend/*\" | head -20","description":"Find backend Python files"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":194,"cache_read_input_tokens":66644,"cache_creation":{"ephemeral_5m_input_tokens":194,"ephemeral_1h_input_tokens":0},"output_tokens":120,"service_tier":"standard"}},"requestId":"req_011CWhmNPaJxaM5U1731NMwE","type":"assistant","uuid":"41898ac8-9514-436c-8411-165e8590951d","timestamp":"2026-01-02T02:30:56.277Z"}
{"parentUuid":"41898ac8-9514-436c-8411-165e8590951d","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01M6rJSeREzXWmRwpfe1fGSw","type":"tool_result","content":"/Users/jeremy/Projects/LLMCouncil/backend/council.py\n/Users/jeremy/Projects/LLMCouncil/backend/config.py\n/Users/jeremy/Projects/LLMCouncil/backend/openrouter.py\n/Users/jeremy/Projects/LLMCouncil/backend/tests/conftest.py\n/Users/jeremy/Projects/LLMCouncil/backend/tests/test_council.py\n/Users/jeremy/Projects/LLMCouncil/backend/tests/__init__.py\n/Users/jeremy/Projects/LLMCouncil/backend/tests/test_openrouter.py\n/Users/jeremy/Projects/LLMCouncil/backend/tests/test_api.py\n/Users/jeremy/Projects/LLMCouncil/backend/__init__.py\n/Users/jeremy/Projects/LLMCouncil/backend/storage.py\n/Users/jeremy/Projects/LLMCouncil/backend/db/models.py\n/Users/jeremy/Projects/LLMCouncil/backend/db/session.py\n/Users/jeremy/Projects/LLMCouncil/backend/db/__init__.py\n/Users/jeremy/Projects/LLMCouncil/backend/main.py","is_error":false}]},"uuid":"90fc0b2a-ff1a-4af8-a650-5e03251bb835","timestamp":"2026-01-02T02:30:57.675Z","toolUseResult":{"stdout":"/Users/jeremy/Projects/LLMCouncil/backend/council.py\n/Users/jeremy/Projects/LLMCouncil/backend/config.py\n/Users/jeremy/Projects/LLMCouncil/backend/openrouter.py\n/Users/jeremy/Projects/LLMCouncil/backend/tests/conftest.py\n/Users/jeremy/Projects/LLMCouncil/backend/tests/test_council.py\n/Users/jeremy/Projects/LLMCouncil/backend/tests/__init__.py\n/Users/jeremy/Projects/LLMCouncil/backend/tests/test_openrouter.py\n/Users/jeremy/Projects/LLMCouncil/backend/tests/test_api.py\n/Users/jeremy/Projects/LLMCouncil/backend/__init__.py\n/Users/jeremy/Projects/LLMCouncil/backend/storage.py\n/Users/jeremy/Projects/LLMCouncil/backend/db/models.py\n/Users/jeremy/Projects/LLMCouncil/backend/db/session.py\n/Users/jeremy/Projects/LLMCouncil/backend/db/__init__.py\n/Users/jeremy/Projects/LLMCouncil/backend/main.py","stderr":"","interrupted":false,"isImage":false}}
{"parentUuid":"90fc0b2a-ff1a-4af8-a650-5e03251bb835","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01YRFbTGbQEPDVCaMrpCtA7F","type":"message","role":"assistant","content":[{"type":"text","text":"Great! Now let me examine the main council.py file and related files to understand the prompt design:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":473,"cache_read_input_tokens":66838,"cache_creation":{"ephemeral_5m_input_tokens":473,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWhmNoAsxioZMgtDHkaB5","type":"assistant","uuid":"6e1b3f8b-ea7d-4ae9-b71e-221e012bd10a","timestamp":"2026-01-02T02:31:01.491Z"}
{"parentUuid":"6e1b3f8b-ea7d-4ae9-b71e-221e012bd10a","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01YRFbTGbQEPDVCaMrpCtA7F","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Pgi1Ezw8MJ64KiKnnCkPh8","name":"Read","input":{"file_path":"/Users/jeremy/Projects/LLMCouncil/backend/council.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":473,"cache_read_input_tokens":66838,"cache_creation":{"ephemeral_5m_input_tokens":473,"ephemeral_1h_input_tokens":0},"output_tokens":93,"service_tier":"standard"}},"requestId":"req_011CWhmNoAsxioZMgtDHkaB5","type":"assistant","uuid":"a07326e8-cbf9-423b-aec6-e141aeab38e0","timestamp":"2026-01-02T02:31:01.682Z"}
{"parentUuid":"a07326e8-cbf9-423b-aec6-e141aeab38e0","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Pgi1Ezw8MJ64KiKnnCkPh8","type":"tool_result","content":"     1→\"\"\"3-stage LLM Council orchestration.\"\"\"\n     2→\n     3→from typing import List, Dict, Any, Tuple, Optional, AsyncGenerator\n     4→from .openrouter import (\n     5→    query_models_parallel,\n     6→    query_model,\n     7→    query_models_parallel_with_circuit_breaker,\n     8→    query_model_with_circuit_breaker,\n     9→    stream_model,\n    10→    CircuitBreaker\n    11→)\n    12→from .config import get_council_models, get_chairman_model, get_model_parameters\n    13→\n    14→\n    15→# OpenRouter pricing per 1M tokens (approximate, varies by model)\n    16→# Format: model_prefix -> (input_price, output_price) per 1M tokens\n    17→MODEL_PRICING = {\n    18→    'openai/gpt-4o': (2.50, 10.00),\n    19→    'openai/gpt-4': (30.00, 60.00),\n    20→    'openai/gpt-3.5': (0.50, 1.50),\n    21→    'anthropic/claude-3.5': (3.00, 15.00),\n    22→    'anthropic/claude-3': (3.00, 15.00),\n    23→    'anthropic/claude-sonnet': (3.00, 15.00),\n    24→    'google/gemini-2.5': (0.15, 0.60),\n    25→    'google/gemini-2': (0.10, 0.40),\n    26→    'google/gemini-1.5': (0.075, 0.30),\n    27→    'x-ai/grok': (5.00, 15.00),\n    28→    'meta-llama/llama': (0.20, 0.20),\n    29→    'mistralai/': (0.25, 0.25),\n    30→    'default': (1.00, 3.00),  # Fallback pricing\n    31→}\n    32→\n    33→\n    34→def estimate_cost(model: str, metrics: Dict[str, Any]) -> float:\n    35→    \"\"\"\n    36→    Estimate the cost of an API call based on token usage.\n    37→\n    38→    Args:\n    39→        model: The model identifier\n    40→        metrics: Dict with input_tokens and output_tokens\n    41→\n    42→    Returns:\n    43→        Estimated cost in USD\n    44→    \"\"\"\n    45→    input_tokens = metrics.get('input_tokens', 0) or 0\n    46→    output_tokens = metrics.get('output_tokens', 0) or 0\n    47→\n    48→    # Find matching pricing\n    49→    input_price, output_price = MODEL_PRICING['default']\n    50→    for prefix, (inp, outp) in MODEL_PRICING.items():\n    51→        if prefix != 'default' and model.startswith(prefix):\n    52→            input_price, output_price = inp, outp\n    53→            break\n    54→\n    55→    # Calculate cost (prices are per 1M tokens)\n    56→    cost = (input_tokens * input_price + output_tokens * output_price) / 1_000_000\n    57→\n    58→    return round(cost, 6)\n    59→\n    60→\n    61→def build_messages_with_history(\n    62→    user_query: str,\n    63→    conversation_history: Optional[List[Dict[str, str]]] = None\n    64→) -> List[Dict[str, str]]:\n    65→    \"\"\"\n    66→    Build messages list including conversation history.\n    67→\n    68→    Args:\n    69→        user_query: The current user query\n    70→        conversation_history: Previous messages in the conversation\n    71→\n    72→    Returns:\n    73→        List of message dicts for the API\n    74→    \"\"\"\n    75→    messages = []\n    76→\n    77→    # Add conversation history if provided\n    78→    if conversation_history:\n    79→        for msg in conversation_history:\n    80→            messages.append({\n    81→                \"role\": msg.get(\"role\", \"user\"),\n    82→                \"content\": msg.get(\"content\", \"\")\n    83→            })\n    84→\n    85→    # Add current query\n    86→    messages.append({\"role\": \"user\", \"content\": user_query})\n    87→\n    88→    return messages\n    89→\n    90→\n    91→def parse_confidence_from_response(response: str) -> Tuple[str, Optional[float]]:\n    92→    \"\"\"\n    93→    Parse confidence score from a response.\n    94→\n    95→    Looks for patterns like:\n    96→    - \"Confidence: 8/10\"\n    97→    - \"Confidence: 85%\"\n    98→    - \"Confidence: High\"\n    99→    - \"[CONFIDENCE: 0.8]\"\n   100→\n   101→    Args:\n   102→        response: The model's response text\n   103→\n   104→    Returns:\n   105→        Tuple of (cleaned response without confidence marker, confidence score 0-1)\n   106→    \"\"\"\n   107→    import re\n   108→\n   109→    if not response:\n   110→        return response, None\n   111→\n   112→    # Pattern 1: [CONFIDENCE: X] at the end (preferred format)\n   113→    match = re.search(r'\\[CONFIDENCE:\\s*(\\d*\\.?\\d+)\\]\\s*$', response)\n   114→    if match:\n   115→        score = float(match.group(1))\n   116→        if score > 1:\n   117→            score = score / 100 if score <= 100 else score / 10\n   118→        cleaned = re.sub(r'\\s*\\[CONFIDENCE:\\s*\\d*\\.?\\d+\\]\\s*$', '', response).strip()\n   119→        return cleaned, min(1.0, max(0.0, score))\n   120→\n   121→    # Pattern 2: \"Confidence: X/10\" or \"Confidence: X%\"\n   122→    match = re.search(r'[Cc]onfidence:\\s*(\\d+(?:\\.\\d+)?)\\s*[/%]?\\s*(?:10|100)?', response)\n   123→    if match:\n   124→        value = float(match.group(1))\n   125→        if value > 1:\n   126→            score = value / 100 if value <= 100 else value / 10\n   127→        else:\n   128→            score = value\n   129→        return response, min(1.0, max(0.0, score))\n   130→\n   131→    # Pattern 3: Word-based confidence\n   132→    confidence_words = {\n   133→        'very high': 0.95, 'extremely high': 0.95,\n   134→        'high': 0.8, 'confident': 0.8,\n   135→        'moderate': 0.6, 'medium': 0.6,\n   136→        'low': 0.4, 'uncertain': 0.3,\n   137→        'very low': 0.2, 'not confident': 0.2,\n   138→    }\n   139→    for word, score in confidence_words.items():\n   140→        if re.search(rf'[Cc]onfidence:\\s*{word}', response, re.IGNORECASE):\n   141→            return response, score\n   142→\n   143→    return response, None\n   144→\n   145→\n   146→async def stage1_collect_responses(\n   147→    user_query: str,\n   148→    conversation_history: Optional[List[Dict[str, str]]] = None,\n   149→    use_circuit_breaker: bool = True,\n   150→    enable_confidence: bool = False\n   151→) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n   152→    \"\"\"\n   153→    Stage 1: Collect individual responses from all council models.\n   154→\n   155→    Args:\n   156→        user_query: The user's question\n   157→        conversation_history: Previous messages for multi-turn context\n   158→        use_circuit_breaker: Whether to use circuit breaker protection\n   159→        enable_confidence: Whether to ask models for confidence scores\n   160→\n   161→    Returns:\n   162→        Tuple of (results list, metrics list)\n   163→    \"\"\"\n   164→    messages = build_messages_with_history(user_query, conversation_history)\n   165→\n   166→    # Add confidence prompt if enabled\n   167→    if enable_confidence:\n   168→        # Find the user message and append confidence instruction\n   169→        for msg in messages:\n   170→            if msg['role'] == 'user':\n   171→                msg['content'] = msg['content'] + '''\n   172→\n   173→Please end your response with a confidence score in this exact format:\n   174→[CONFIDENCE: X.X]\n   175→Where X.X is a number between 0.0 (not confident) and 1.0 (very confident).'''\n   176→                break\n   177→\n   178→    # Query all models in parallel\n   179→    council_models = get_council_models()\n   180→    model_params = get_model_parameters()\n   181→\n   182→    if use_circuit_breaker:\n   183→        responses = await query_models_parallel_with_circuit_breaker(council_models, messages, model_params)\n   184→    else:\n   185→        responses = await query_models_parallel(council_models, messages, model_params)\n   186→\n   187→    # Format results and collect metrics\n   188→    stage1_results = []\n   189→    stage1_metrics = []\n   190→\n   191→    for model, response in responses.items():\n   192→        if response is not None:\n   193→            content = response.get('content')\n   194→            metrics = response.get('metrics', {})\n   195→\n   196→            # Build metrics record\n   197→            metric_record = {\n   198→                'model_id': model,\n   199→                'stage': 'stage1',\n   200→                'response_time_ms': metrics.get('response_time_ms'),\n   201→                'input_tokens': metrics.get('input_tokens'),\n   202→                'output_tokens': metrics.get('output_tokens'),\n   203→                'total_tokens': metrics.get('total_tokens'),\n   204→                'error_type': metrics.get('error_type'),\n   205→            }\n   206→\n   207→            # Calculate estimated cost\n   208→            if metrics.get('total_tokens'):\n   209→                metric_record['cost_usd'] = estimate_cost(model, metrics)\n   210→\n   211→            stage1_metrics.append(metric_record)\n   212→\n   213→            # Only include in results if we got content\n   214→            if content:\n   215→                result = {\n   216→                    \"model\": model,\n   217→                    \"response\": content\n   218→                }\n   219→\n   220→                # Parse confidence score if enabled\n   221→                if enable_confidence:\n   222→                    cleaned_response, confidence = parse_confidence_from_response(content)\n   223→                    result[\"response\"] = cleaned_response\n   224→                    result[\"confidence\"] = confidence\n   225→\n   226→                stage1_results.append(result)\n   227→\n   228→    return stage1_results, stage1_metrics\n   229→\n   230→\n   231→def build_ranking_prompt(\n   232→    user_query: str,\n   233→    responses_text: str,\n   234→    ranking_criteria: Optional[List[Dict[str, Any]]] = None\n   235→) -> str:\n   236→    \"\"\"\n   237→    Build the Stage 2 ranking prompt with optional custom criteria.\n   238→\n   239→    Args:\n   240→        user_query: The original user question\n   241→        responses_text: Formatted anonymized responses\n   242→        ranking_criteria: Optional list of criteria to evaluate against\n   243→\n   244→    Returns:\n   245→        The formatted ranking prompt\n   246→    \"\"\"\n   247→    # Build criteria section if custom criteria provided\n   248→    criteria_text = \"\"\n   249→    if ranking_criteria:\n   250→        enabled_criteria = [c for c in ranking_criteria if c.get('enabled', True)]\n   251→        if enabled_criteria:\n   252→            criteria_list = \"\\n\".join([\n   253→                f\"- **{c['name']}**: {c.get('description', '')}\"\n   254→                for c in enabled_criteria\n   255→            ])\n   256→            criteria_text = f\"\"\"\n   257→Evaluate each response based on these criteria:\n   258→{criteria_list}\n   259→\n   260→\"\"\"\n   261→\n   262→    return f\"\"\"You are evaluating different responses to the following question:\n   263→\n   264→Question: {user_query}\n   265→\n   266→Here are the responses from different models (anonymized):\n   267→\n   268→{responses_text}\n   269→\n   270→Your task:\n   271→1. First, evaluate each response individually.{criteria_text} For each response, explain what it does well and what it does poorly.\n   272→2. Then, at the very end of your response, provide a final ranking.\n   273→\n   274→IMPORTANT: Your final ranking MUST be formatted EXACTLY as follows:\n   275→- Start with the line \"FINAL RANKING:\" (all caps, with colon)\n   276→- Then list the responses from best to worst as a numbered list\n   277→- Each line should be: number, period, space, then ONLY the response label (e.g., \"1. Response A\")\n   278→- Do not add any other text or explanations in the ranking section\n   279→\n   280→Example of the correct format for your ENTIRE response:\n   281→\n   282→Response A provides good detail on X but misses Y...\n   283→Response B is accurate but lacks depth on Z...\n   284→Response C offers the most comprehensive answer...\n   285→\n   286→FINAL RANKING:\n   287→1. Response C\n   288→2. Response A\n   289→3. Response B\n   290→\n   291→Now provide your evaluation and ranking:\"\"\"\n   292→\n   293→\n   294→async def stage2_collect_rankings(\n   295→    user_query: str,\n   296→    stage1_results: List[Dict[str, Any]],\n   297→    use_circuit_breaker: bool = True,\n   298→    ranking_criteria: Optional[List[Dict[str, Any]]] = None\n   299→) -> Tuple[List[Dict[str, Any]], Dict[str, str], List[Dict[str, Any]]]:\n   300→    \"\"\"\n   301→    Stage 2: Each model ranks the anonymized responses.\n   302→\n   303→    Args:\n   304→        user_query: The original user query\n   305→        stage1_results: Results from Stage 1\n   306→        use_circuit_breaker: Whether to use circuit breaker protection\n   307→        ranking_criteria: Optional custom ranking criteria\n   308→\n   309→    Returns:\n   310→        Tuple of (rankings list, label_to_model mapping, metrics list)\n   311→    \"\"\"\n   312→    # Create anonymized labels for responses (Response A, Response B, etc.)\n   313→    labels = [chr(65 + i) for i in range(len(stage1_results))]  # A, B, C, ...\n   314→\n   315→    # Create mapping from label to model name\n   316→    label_to_model = {\n   317→        f\"Response {label}\": result['model']\n   318→        for label, result in zip(labels, stage1_results)\n   319→    }\n   320→\n   321→    # Build the ranking prompt\n   322→    responses_text = \"\\n\\n\".join([\n   323→        f\"Response {label}:\\n{result['response']}\"\n   324→        for label, result in zip(labels, stage1_results)\n   325→    ])\n   326→\n   327→    ranking_prompt = build_ranking_prompt(user_query, responses_text, ranking_criteria)\n   328→\n   329→    messages = [{\"role\": \"user\", \"content\": ranking_prompt}]\n   330→\n   331→    # Get rankings from all council models in parallel\n   332→    council_models = get_council_models()\n   333→    model_params = get_model_parameters()\n   334→\n   335→    if use_circuit_breaker:\n   336→        responses = await query_models_parallel_with_circuit_breaker(council_models, messages, model_params)\n   337→    else:\n   338→        responses = await query_models_parallel(council_models, messages, model_params)\n   339→\n   340→    # Format results and collect metrics\n   341→    stage2_results = []\n   342→    stage2_metrics = []\n   343→\n   344→    for model, response in responses.items():\n   345→        if response is not None:\n   346→            full_text = response.get('content', '')\n   347→            metrics = response.get('metrics', {})\n   348→\n   349→            # Build metrics record\n   350→            metric_record = {\n   351→                'model_id': model,\n   352→                'stage': 'stage2',\n   353→                'response_time_ms': metrics.get('response_time_ms'),\n   354→                'input_tokens': metrics.get('input_tokens'),\n   355→                'output_tokens': metrics.get('output_tokens'),\n   356→                'total_tokens': metrics.get('total_tokens'),\n   357→                'error_type': metrics.get('error_type'),\n   358→            }\n   359→\n   360→            if metrics.get('total_tokens'):\n   361→                metric_record['cost_usd'] = estimate_cost(model, metrics)\n   362→\n   363→            stage2_metrics.append(metric_record)\n   364→\n   365→            if full_text:\n   366→                parsed = parse_ranking_from_text(full_text)\n   367→                stage2_results.append({\n   368→                    \"model\": model,\n   369→                    \"ranking\": full_text,\n   370→                    \"parsed_ranking\": parsed\n   371→                })\n   372→\n   373→    return stage2_results, label_to_model, stage2_metrics\n   374→\n   375→\n   376→def build_chairman_prompt(\n   377→    user_query: str,\n   378→    stage1_results: List[Dict[str, Any]],\n   379→    stage2_results: List[Dict[str, Any]]\n   380→) -> str:\n   381→    \"\"\"Build the chairman synthesis prompt.\"\"\"\n   382→    stage1_text = \"\\n\\n\".join([\n   383→        f\"Model: {result['model']}\\nResponse: {result['response']}\"\n   384→        for result in stage1_results\n   385→    ])\n   386→\n   387→    stage2_text = \"\\n\\n\".join([\n   388→        f\"Model: {result['model']}\\nRanking: {result['ranking']}\"\n   389→        for result in stage2_results\n   390→    ])\n   391→\n   392→    return f\"\"\"You are the Chairman of an LLM Council. Multiple AI models have provided responses to a user's question, and then ranked each other's responses.\n   393→\n   394→Original Question: {user_query}\n   395→\n   396→STAGE 1 - Individual Responses:\n   397→{stage1_text}\n   398→\n   399→STAGE 2 - Peer Rankings:\n   400→{stage2_text}\n   401→\n   402→Your task as Chairman is to synthesize all of this information into a single, comprehensive, accurate answer to the user's original question. Consider:\n   403→- The individual responses and their insights\n   404→- The peer rankings and what they reveal about response quality\n   405→- Any patterns of agreement or disagreement\n   406→\n   407→Provide a clear, well-reasoned final answer that represents the council's collective wisdom:\"\"\"\n   408→\n   409→\n   410→async def stage3_synthesize_final(\n   411→    user_query: str,\n   412→    stage1_results: List[Dict[str, Any]],\n   413→    stage2_results: List[Dict[str, Any]],\n   414→    use_circuit_breaker: bool = True\n   415→) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n   416→    \"\"\"\n   417→    Stage 3: Chairman synthesizes final response.\n   418→\n   419→    Args:\n   420→        user_query: The original user query\n   421→        stage1_results: Individual model responses from Stage 1\n   422→        stage2_results: Rankings from Stage 2\n   423→        use_circuit_breaker: Whether to use circuit breaker protection\n   424→\n   425→    Returns:\n   426→        Tuple of (result dict, metrics dict)\n   427→    \"\"\"\n   428→    chairman_prompt = build_chairman_prompt(user_query, stage1_results, stage2_results)\n   429→    messages = [{\"role\": \"user\", \"content\": chairman_prompt}]\n   430→\n   431→    # Query the chairman model\n   432→    chairman_model = get_chairman_model()\n   433→    model_params = get_model_parameters()\n   434→\n   435→    # Get chairman-specific parameters\n   436→    chairman_params = {}\n   437→    if chairman_model in model_params:\n   438→        mp = model_params[chairman_model]\n   439→        if \"temperature\" in mp:\n   440→            chairman_params[\"temperature\"] = mp[\"temperature\"]\n   441→        if \"max_tokens\" in mp:\n   442→            chairman_params[\"max_tokens\"] = mp[\"max_tokens\"]\n   443→\n   444→    if use_circuit_breaker:\n   445→        response = await query_model_with_circuit_breaker(chairman_model, messages, **chairman_params)\n   446→    else:\n   447→        response = await query_model(chairman_model, messages, **chairman_params)\n   448→\n   449→    # Build metrics\n   450→    metrics = response.get('metrics', {}) if response else {}\n   451→    stage3_metrics = {\n   452→        'model_id': chairman_model,\n   453→        'stage': 'stage3',\n   454→        'response_time_ms': metrics.get('response_time_ms'),\n   455→        'input_tokens': metrics.get('input_tokens'),\n   456→        'output_tokens': metrics.get('output_tokens'),\n   457→        'total_tokens': metrics.get('total_tokens'),\n   458→        'error_type': metrics.get('error_type'),\n   459→    }\n   460→\n   461→    if metrics.get('total_tokens'):\n   462→        stage3_metrics['cost_usd'] = estimate_cost(chairman_model, metrics)\n   463→\n   464→    if response is None or not response.get('content'):\n   465→        # Fallback if chairman fails\n   466→        return {\n   467→            \"model\": chairman_model,\n   468→            \"response\": \"Error: Unable to generate final synthesis.\"\n   469→        }, stage3_metrics\n   470→\n   471→    return {\n   472→        \"model\": chairman_model,\n   473→        \"response\": response.get('content', '')\n   474→    }, stage3_metrics\n   475→\n   476→\n   477→async def stage3_synthesize_streaming(\n   478→    user_query: str,\n   479→    stage1_results: List[Dict[str, Any]],\n   480→    stage2_results: List[Dict[str, Any]]\n   481→) -> AsyncGenerator[Dict[str, Any], None]:\n   482→    \"\"\"\n   483→    Stage 3: Chairman synthesizes final response with token streaming.\n   484→\n   485→    Yields:\n   486→        Dict with 'type' (token, done, error), 'model', and 'content'\n   487→    \"\"\"\n   488→    chairman_prompt = build_chairman_prompt(user_query, stage1_results, stage2_results)\n   489→    messages = [{\"role\": \"user\", \"content\": chairman_prompt}]\n   490→\n   491→    chairman_model = get_chairman_model()\n   492→    model_params = get_model_parameters()\n   493→\n   494→    # Get chairman-specific parameters for streaming\n   495→    stream_params = {}\n   496→    if chairman_model in model_params:\n   497→        mp = model_params[chairman_model]\n   498→        if \"temperature\" in mp:\n   499→            stream_params[\"temperature\"] = mp[\"temperature\"]\n   500→        if \"max_tokens\" in mp:\n   501→            stream_params[\"max_tokens\"] = mp[\"max_tokens\"]\n   502→\n   503→    # Check circuit breaker\n   504→    if not CircuitBreaker.can_execute(chairman_model):\n   505→        yield {\n   506→            \"type\": \"error\",\n   507→            \"model\": chairman_model,\n   508→            \"content\": \"Circuit breaker open - model temporarily unavailable\"\n   509→        }\n   510→        return\n   511→\n   512→    async for chunk in stream_model(chairman_model, messages, **stream_params):\n   513→        yield {\n   514→            \"type\": chunk[\"type\"],\n   515→            \"model\": chairman_model,\n   516→            \"content\": chunk[\"content\"]\n   517→        }\n   518→\n   519→        # Record success/failure based on streaming result\n   520→        if chunk[\"type\"] == \"done\":\n   521→            CircuitBreaker.record_success(chairman_model)\n   522→        elif chunk[\"type\"] == \"error\":\n   523→            CircuitBreaker.record_failure(chairman_model)\n   524→\n   525→\n   526→def parse_ranking_from_text(ranking_text: str) -> List[str]:\n   527→    \"\"\"\n   528→    Parse the FINAL RANKING section from the model's response.\n   529→\n   530→    Args:\n   531→        ranking_text: The full text response from the model\n   532→\n   533→    Returns:\n   534→        List of response labels in ranked order\n   535→    \"\"\"\n   536→    import re\n   537→\n   538→    # Look for \"FINAL RANKING:\" section\n   539→    if \"FINAL RANKING:\" in ranking_text:\n   540→        # Extract everything after \"FINAL RANKING:\"\n   541→        parts = ranking_text.split(\"FINAL RANKING:\")\n   542→        if len(parts) >= 2:\n   543→            ranking_section = parts[1]\n   544→            # Try to extract numbered list format (e.g., \"1. Response A\")\n   545→            # This pattern looks for: number, period, optional space, \"Response X\"\n   546→            numbered_matches = re.findall(r'\\d+\\.\\s*Response [A-Z]', ranking_section)\n   547→            if numbered_matches:\n   548→                # Extract just the \"Response X\" part\n   549→                return [re.search(r'Response [A-Z]', m).group() for m in numbered_matches]\n   550→\n   551→            # Fallback: Extract all \"Response X\" patterns in order\n   552→            matches = re.findall(r'Response [A-Z]', ranking_section)\n   553→            return matches\n   554→\n   555→    # Fallback: try to find any \"Response X\" patterns in order\n   556→    matches = re.findall(r'Response [A-Z]', ranking_text)\n   557→    return matches\n   558→\n   559→\n   560→def calculate_aggregate_rankings(\n   561→    stage2_results: List[Dict[str, Any]],\n   562→    label_to_model: Dict[str, str],\n   563→    model_weights: Optional[Dict[str, float]] = None\n   564→) -> List[Dict[str, Any]]:\n   565→    \"\"\"\n   566→    Calculate aggregate rankings across all models with optional weighted voting.\n   567→\n   568→    Args:\n   569→        stage2_results: Rankings from each model\n   570→        label_to_model: Mapping from anonymous labels to model names\n   571→        model_weights: Optional dict mapping model_id to voting weight (default 1.0)\n   572→\n   573→    Returns:\n   574→        List of dicts with model name and average rank, sorted best to worst\n   575→    \"\"\"\n   576→    from collections import defaultdict\n   577→\n   578→    # Track positions for each model with weights\n   579→    model_positions = defaultdict(list)\n   580→    model_weighted_positions = defaultdict(list)\n   581→\n   582→    for ranking in stage2_results:\n   583→        ranking_text = ranking['ranking']\n   584→        ranker_model = ranking.get('model', '')\n   585→\n   586→        # Get weight for the ranking model (default 1.0)\n   587→        weight = 1.0\n   588→        if model_weights and ranker_model in model_weights:\n   589→            weight = model_weights[ranker_model]\n   590→\n   591→        # Parse the ranking from the structured format\n   592→        parsed_ranking = parse_ranking_from_text(ranking_text)\n   593→\n   594→        for position, label in enumerate(parsed_ranking, start=1):\n   595→            if label in label_to_model:\n   596→                model_name = label_to_model[label]\n   597→                model_positions[model_name].append(position)\n   598→                model_weighted_positions[model_name].append((position, weight))\n   599→\n   600→    # Calculate average position for each model\n   601→    aggregate = []\n   602→    for model, positions in model_positions.items():\n   603→        if positions:\n   604→            # Calculate unweighted average\n   605→            avg_rank = sum(positions) / len(positions)\n   606→\n   607→            # Calculate weighted average if weights provided\n   608→            weighted_positions = model_weighted_positions[model]\n   609→            total_weight = sum(w for _, w in weighted_positions)\n   610→            weighted_avg = sum(p * w for p, w in weighted_positions) / total_weight if total_weight > 0 else avg_rank\n   611→\n   612→            aggregate.append({\n   613→                \"model\": model,\n   614→                \"average_rank\": round(weighted_avg, 2),\n   615→                \"unweighted_rank\": round(avg_rank, 2),\n   616→                \"rankings_count\": len(positions)\n   617→            })\n   618→\n   619→    # Sort by average rank (lower is better)\n   620→    aggregate.sort(key=lambda x: x['average_rank'])\n   621→\n   622→    return aggregate\n   623→\n   624→\n   625→def calculate_dissent_metrics(\n   626→    stage2_results: List[Dict[str, Any]],\n   627→    label_to_model: Dict[str, str]\n   628→) -> Dict[str, Any]:\n   629→    \"\"\"\n   630→    Calculate disagreement/dissent metrics from Stage 2 rankings.\n   631→\n   632→    Args:\n   633→        stage2_results: Rankings from each model\n   634→        label_to_model: Mapping from anonymous labels to model names\n   635→\n   636→    Returns:\n   637→        Dict with dissent metrics: agreement_score, controversies, unanimous_winner\n   638→    \"\"\"\n   639→    from collections import defaultdict\n   640→\n   641→    if len(stage2_results) < 2:\n   642→        return {\"agreement_score\": 1.0, \"controversies\": [], \"unanimous_winner\": None}\n   643→\n   644→    # Track who each model ranked first and last\n   645→    first_place_votes = defaultdict(list)  # model -> list of rankers who put them first\n   646→    last_place_votes = defaultdict(list)   # model -> list of rankers who put them last\n   647→    all_rankings = []  # list of (ranker, [ranked models in order])\n   648→\n   649→    for ranking in stage2_results:\n   650→        ranker = ranking.get('model', 'unknown')\n   651→        parsed = parse_ranking_from_text(ranking['ranking'])\n   652→\n   653→        if parsed:\n   654→            # Convert labels to model names\n   655→            ranked_models = [label_to_model.get(label, label) for label in parsed]\n   656→            all_rankings.append((ranker, ranked_models))\n   657→\n   658→            if ranked_models:\n   659→                first_place_votes[ranked_models[0]].append(ranker)\n   660→                last_place_votes[ranked_models[-1]].append(ranker)\n   661→\n   662→    # Calculate agreement score (0-1, 1 = perfect agreement)\n   663→    # Based on Kendall's W (coefficient of concordance) simplified\n   664→    num_rankers = len(all_rankings)\n   665→    if num_rankers < 2:\n   666→        return {\"agreement_score\": 1.0, \"controversies\": [], \"unanimous_winner\": None}\n   667→\n   668→    # Check for unanimous first place\n   669→    unanimous_winner = None\n   670→    for model, voters in first_place_votes.items():\n   671→        if len(voters) == num_rankers:\n   672→            unanimous_winner = model\n   673→            break\n   674→\n   675→    # Find controversial responses (models with both first and last place votes)\n   676→    controversies = []\n   677→    for model in set(first_place_votes.keys()) | set(last_place_votes.keys()):\n   678→        first_votes = len(first_place_votes.get(model, []))\n   679→        last_votes = len(last_place_votes.get(model, []))\n   680→        if first_votes > 0 and last_votes > 0:\n   681→            controversies.append({\n   682→                \"model\": model,\n   683→                \"first_place_votes\": first_votes,\n   684→                \"last_place_votes\": last_votes,\n   685→                \"controversy_score\": round((first_votes + last_votes) / num_rankers, 2)\n   686→            })\n   687→\n   688→    # Sort controversies by controversy score\n   689→    controversies.sort(key=lambda x: x['controversy_score'], reverse=True)\n   690→\n   691→    # Calculate simple agreement score based on position variance\n   692→    # Lower variance = higher agreement\n   693→    from collections import defaultdict\n   694→    position_lists = defaultdict(list)\n   695→    for ranker, ranked_models in all_rankings:\n   696→        for pos, model in enumerate(ranked_models, start=1):\n   697→            position_lists[model].append(pos)\n   698→\n   699→    if position_lists:\n   700→        variances = []\n   701→        for model, positions in position_lists.items():\n   702→            if len(positions) > 1:\n   703→                mean = sum(positions) / len(positions)\n   704→                variance = sum((p - mean) ** 2 for p in positions) / len(positions)\n   705→                variances.append(variance)\n   706→\n   707→        if variances:\n   708→            avg_variance = sum(variances) / len(variances)\n   709→            # Normalize: variance of 0 = agreement 1.0, high variance = lower agreement\n   710→            # Max possible variance for n items is roughly (n-1)^2 / 4\n   711→            max_variance = (len(label_to_model) - 1) ** 2 / 4 if len(label_to_model) > 1 else 1\n   712→            agreement_score = max(0, 1 - (avg_variance / max_variance)) if max_variance > 0 else 1.0\n   713→        else:\n   714→            agreement_score = 1.0\n   715→    else:\n   716→        agreement_score = 1.0\n   717→\n   718→    return {\n   719→        \"agreement_score\": round(agreement_score, 2),\n   720→        \"controversies\": controversies,\n   721→        \"unanimous_winner\": unanimous_winner\n   722→    }\n   723→\n   724→\n   725→async def generate_conversation_title(user_query: str) -> str:\n   726→    \"\"\"\n   727→    Generate a short title for a conversation based on the first user message.\n   728→\n   729→    Args:\n   730→        user_query: The first user message\n   731→\n   732→    Returns:\n   733→        A short title (3-5 words)\n   734→    \"\"\"\n   735→    title_prompt = f\"\"\"Generate a very short title (3-5 words maximum) that summarizes the following question.\n   736→The title should be concise and descriptive. Do not use quotes or punctuation in the title.\n   737→\n   738→Question: {user_query}\n   739→\n   740→Title:\"\"\"\n   741→\n   742→    messages = [{\"role\": \"user\", \"content\": title_prompt}]\n   743→\n   744→    # Use gemini-2.5-flash for title generation (fast and cheap)\n   745→    response = await query_model(\"google/gemini-2.5-flash\", messages, timeout=30.0)\n   746→\n   747→    if response is None or response.get('content') is None:\n   748→        # Fallback to a generic title\n   749→        return \"New Conversation\"\n   750→\n   751→    title = response.get('content', 'New Conversation').strip()\n   752→\n   753→    # Clean up the title - remove quotes, limit length\n   754→    title = title.strip('\"\\'')\n   755→\n   756→    # Truncate if too long\n   757→    if len(title) > 50:\n   758→        title = title[:47] + \"...\"\n   759→\n   760→    return title\n   761→\n   762→\n   763→async def run_full_council(\n   764→    user_query: str,\n   765→    ranking_criteria: Optional[List[Dict[str, Any]]] = None,\n   766→    model_weights: Optional[Dict[str, float]] = None,\n   767→    enable_dissent_tracking: bool = True,\n   768→    enable_confidence: bool = False\n   769→) -> Tuple[List, List, Dict, Dict, List]:\n   770→    \"\"\"\n   771→    Run the complete 3-stage council process.\n   772→\n   773→    Args:\n   774→        user_query: The user's question\n   775→        ranking_criteria: Optional custom ranking criteria for Stage 2\n   776→        model_weights: Optional model voting weights for aggregate rankings\n   777→        enable_dissent_tracking: Whether to calculate dissent metrics\n   778→        enable_confidence: Whether to ask models for confidence scores\n   779→\n   780→    Returns:\n   781→        Tuple of (stage1_results, stage2_results, stage3_result, metadata, all_metrics)\n   782→    \"\"\"\n   783→    all_metrics = []\n   784→\n   785→    # Stage 1: Collect individual responses\n   786→    stage1_results, stage1_metrics = await stage1_collect_responses(\n   787→        user_query,\n   788→        enable_confidence=enable_confidence\n   789→    )\n   790→    all_metrics.extend(stage1_metrics)\n   791→\n   792→    # If no models responded successfully, return error\n   793→    if not stage1_results:\n   794→        return [], [], {\n   795→            \"model\": \"error\",\n   796→            \"response\": \"All models failed to respond. Please try again.\"\n   797→        }, {}, all_metrics\n   798→\n   799→    # Stage 2: Collect rankings with optional custom criteria\n   800→    stage2_results, label_to_model, stage2_metrics = await stage2_collect_rankings(\n   801→        user_query,\n   802→        stage1_results,\n   803→        ranking_criteria=ranking_criteria\n   804→    )\n   805→    all_metrics.extend(stage2_metrics)\n   806→\n   807→    # Calculate aggregate rankings with optional weighted voting\n   808→    aggregate_rankings = calculate_aggregate_rankings(\n   809→        stage2_results,\n   810→        label_to_model,\n   811→        model_weights=model_weights\n   812→    )\n   813→\n   814→    # Add ranking positions to stage1 metrics\n   815→    for ranking in aggregate_rankings:\n   816→        for metric in all_metrics:\n   817→            if metric['model_id'] == ranking['model'] and metric['stage'] == 'stage1':\n   818→                metric['ranking_position'] = ranking['average_rank']\n   819→\n   820→    # Stage 3: Synthesize final answer\n   821→    stage3_result, stage3_metrics = await stage3_synthesize_final(\n   822→        user_query,\n   823→        stage1_results,\n   824→        stage2_results\n   825→    )\n   826→    all_metrics.append(stage3_metrics)\n   827→\n   828→    # Prepare metadata\n   829→    metadata = {\n   830→        \"label_to_model\": label_to_model,\n   831→        \"aggregate_rankings\": aggregate_rankings\n   832→    }\n   833→\n   834→    # Calculate dissent metrics if enabled\n   835→    if enable_dissent_tracking and len(stage2_results) >= 2:\n   836→        dissent_metrics = calculate_dissent_metrics(stage2_results, label_to_model)\n   837→        metadata[\"dissent\"] = dissent_metrics\n   838→\n   839→    return stage1_results, stage2_results, stage3_result, metadata, all_metrics\n   840→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"f589f3cf-0784-4206-86c4-e29efce9ab91","timestamp":"2026-01-02T02:31:02.214Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/jeremy/Projects/LLMCouncil/backend/council.py","content":"\"\"\"3-stage LLM Council orchestration.\"\"\"\n\nfrom typing import List, Dict, Any, Tuple, Optional, AsyncGenerator\nfrom .openrouter import (\n    query_models_parallel,\n    query_model,\n    query_models_parallel_with_circuit_breaker,\n    query_model_with_circuit_breaker,\n    stream_model,\n    CircuitBreaker\n)\nfrom .config import get_council_models, get_chairman_model, get_model_parameters\n\n\n# OpenRouter pricing per 1M tokens (approximate, varies by model)\n# Format: model_prefix -> (input_price, output_price) per 1M tokens\nMODEL_PRICING = {\n    'openai/gpt-4o': (2.50, 10.00),\n    'openai/gpt-4': (30.00, 60.00),\n    'openai/gpt-3.5': (0.50, 1.50),\n    'anthropic/claude-3.5': (3.00, 15.00),\n    'anthropic/claude-3': (3.00, 15.00),\n    'anthropic/claude-sonnet': (3.00, 15.00),\n    'google/gemini-2.5': (0.15, 0.60),\n    'google/gemini-2': (0.10, 0.40),\n    'google/gemini-1.5': (0.075, 0.30),\n    'x-ai/grok': (5.00, 15.00),\n    'meta-llama/llama': (0.20, 0.20),\n    'mistralai/': (0.25, 0.25),\n    'default': (1.00, 3.00),  # Fallback pricing\n}\n\n\ndef estimate_cost(model: str, metrics: Dict[str, Any]) -> float:\n    \"\"\"\n    Estimate the cost of an API call based on token usage.\n\n    Args:\n        model: The model identifier\n        metrics: Dict with input_tokens and output_tokens\n\n    Returns:\n        Estimated cost in USD\n    \"\"\"\n    input_tokens = metrics.get('input_tokens', 0) or 0\n    output_tokens = metrics.get('output_tokens', 0) or 0\n\n    # Find matching pricing\n    input_price, output_price = MODEL_PRICING['default']\n    for prefix, (inp, outp) in MODEL_PRICING.items():\n        if prefix != 'default' and model.startswith(prefix):\n            input_price, output_price = inp, outp\n            break\n\n    # Calculate cost (prices are per 1M tokens)\n    cost = (input_tokens * input_price + output_tokens * output_price) / 1_000_000\n\n    return round(cost, 6)\n\n\ndef build_messages_with_history(\n    user_query: str,\n    conversation_history: Optional[List[Dict[str, str]]] = None\n) -> List[Dict[str, str]]:\n    \"\"\"\n    Build messages list including conversation history.\n\n    Args:\n        user_query: The current user query\n        conversation_history: Previous messages in the conversation\n\n    Returns:\n        List of message dicts for the API\n    \"\"\"\n    messages = []\n\n    # Add conversation history if provided\n    if conversation_history:\n        for msg in conversation_history:\n            messages.append({\n                \"role\": msg.get(\"role\", \"user\"),\n                \"content\": msg.get(\"content\", \"\")\n            })\n\n    # Add current query\n    messages.append({\"role\": \"user\", \"content\": user_query})\n\n    return messages\n\n\ndef parse_confidence_from_response(response: str) -> Tuple[str, Optional[float]]:\n    \"\"\"\n    Parse confidence score from a response.\n\n    Looks for patterns like:\n    - \"Confidence: 8/10\"\n    - \"Confidence: 85%\"\n    - \"Confidence: High\"\n    - \"[CONFIDENCE: 0.8]\"\n\n    Args:\n        response: The model's response text\n\n    Returns:\n        Tuple of (cleaned response without confidence marker, confidence score 0-1)\n    \"\"\"\n    import re\n\n    if not response:\n        return response, None\n\n    # Pattern 1: [CONFIDENCE: X] at the end (preferred format)\n    match = re.search(r'\\[CONFIDENCE:\\s*(\\d*\\.?\\d+)\\]\\s*$', response)\n    if match:\n        score = float(match.group(1))\n        if score > 1:\n            score = score / 100 if score <= 100 else score / 10\n        cleaned = re.sub(r'\\s*\\[CONFIDENCE:\\s*\\d*\\.?\\d+\\]\\s*$', '', response).strip()\n        return cleaned, min(1.0, max(0.0, score))\n\n    # Pattern 2: \"Confidence: X/10\" or \"Confidence: X%\"\n    match = re.search(r'[Cc]onfidence:\\s*(\\d+(?:\\.\\d+)?)\\s*[/%]?\\s*(?:10|100)?', response)\n    if match:\n        value = float(match.group(1))\n        if value > 1:\n            score = value / 100 if value <= 100 else value / 10\n        else:\n            score = value\n        return response, min(1.0, max(0.0, score))\n\n    # Pattern 3: Word-based confidence\n    confidence_words = {\n        'very high': 0.95, 'extremely high': 0.95,\n        'high': 0.8, 'confident': 0.8,\n        'moderate': 0.6, 'medium': 0.6,\n        'low': 0.4, 'uncertain': 0.3,\n        'very low': 0.2, 'not confident': 0.2,\n    }\n    for word, score in confidence_words.items():\n        if re.search(rf'[Cc]onfidence:\\s*{word}', response, re.IGNORECASE):\n            return response, score\n\n    return response, None\n\n\nasync def stage1_collect_responses(\n    user_query: str,\n    conversation_history: Optional[List[Dict[str, str]]] = None,\n    use_circuit_breaker: bool = True,\n    enable_confidence: bool = False\n) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n    \"\"\"\n    Stage 1: Collect individual responses from all council models.\n\n    Args:\n        user_query: The user's question\n        conversation_history: Previous messages for multi-turn context\n        use_circuit_breaker: Whether to use circuit breaker protection\n        enable_confidence: Whether to ask models for confidence scores\n\n    Returns:\n        Tuple of (results list, metrics list)\n    \"\"\"\n    messages = build_messages_with_history(user_query, conversation_history)\n\n    # Add confidence prompt if enabled\n    if enable_confidence:\n        # Find the user message and append confidence instruction\n        for msg in messages:\n            if msg['role'] == 'user':\n                msg['content'] = msg['content'] + '''\n\nPlease end your response with a confidence score in this exact format:\n[CONFIDENCE: X.X]\nWhere X.X is a number between 0.0 (not confident) and 1.0 (very confident).'''\n                break\n\n    # Query all models in parallel\n    council_models = get_council_models()\n    model_params = get_model_parameters()\n\n    if use_circuit_breaker:\n        responses = await query_models_parallel_with_circuit_breaker(council_models, messages, model_params)\n    else:\n        responses = await query_models_parallel(council_models, messages, model_params)\n\n    # Format results and collect metrics\n    stage1_results = []\n    stage1_metrics = []\n\n    for model, response in responses.items():\n        if response is not None:\n            content = response.get('content')\n            metrics = response.get('metrics', {})\n\n            # Build metrics record\n            metric_record = {\n                'model_id': model,\n                'stage': 'stage1',\n                'response_time_ms': metrics.get('response_time_ms'),\n                'input_tokens': metrics.get('input_tokens'),\n                'output_tokens': metrics.get('output_tokens'),\n                'total_tokens': metrics.get('total_tokens'),\n                'error_type': metrics.get('error_type'),\n            }\n\n            # Calculate estimated cost\n            if metrics.get('total_tokens'):\n                metric_record['cost_usd'] = estimate_cost(model, metrics)\n\n            stage1_metrics.append(metric_record)\n\n            # Only include in results if we got content\n            if content:\n                result = {\n                    \"model\": model,\n                    \"response\": content\n                }\n\n                # Parse confidence score if enabled\n                if enable_confidence:\n                    cleaned_response, confidence = parse_confidence_from_response(content)\n                    result[\"response\"] = cleaned_response\n                    result[\"confidence\"] = confidence\n\n                stage1_results.append(result)\n\n    return stage1_results, stage1_metrics\n\n\ndef build_ranking_prompt(\n    user_query: str,\n    responses_text: str,\n    ranking_criteria: Optional[List[Dict[str, Any]]] = None\n) -> str:\n    \"\"\"\n    Build the Stage 2 ranking prompt with optional custom criteria.\n\n    Args:\n        user_query: The original user question\n        responses_text: Formatted anonymized responses\n        ranking_criteria: Optional list of criteria to evaluate against\n\n    Returns:\n        The formatted ranking prompt\n    \"\"\"\n    # Build criteria section if custom criteria provided\n    criteria_text = \"\"\n    if ranking_criteria:\n        enabled_criteria = [c for c in ranking_criteria if c.get('enabled', True)]\n        if enabled_criteria:\n            criteria_list = \"\\n\".join([\n                f\"- **{c['name']}**: {c.get('description', '')}\"\n                for c in enabled_criteria\n            ])\n            criteria_text = f\"\"\"\nEvaluate each response based on these criteria:\n{criteria_list}\n\n\"\"\"\n\n    return f\"\"\"You are evaluating different responses to the following question:\n\nQuestion: {user_query}\n\nHere are the responses from different models (anonymized):\n\n{responses_text}\n\nYour task:\n1. First, evaluate each response individually.{criteria_text} For each response, explain what it does well and what it does poorly.\n2. Then, at the very end of your response, provide a final ranking.\n\nIMPORTANT: Your final ranking MUST be formatted EXACTLY as follows:\n- Start with the line \"FINAL RANKING:\" (all caps, with colon)\n- Then list the responses from best to worst as a numbered list\n- Each line should be: number, period, space, then ONLY the response label (e.g., \"1. Response A\")\n- Do not add any other text or explanations in the ranking section\n\nExample of the correct format for your ENTIRE response:\n\nResponse A provides good detail on X but misses Y...\nResponse B is accurate but lacks depth on Z...\nResponse C offers the most comprehensive answer...\n\nFINAL RANKING:\n1. Response C\n2. Response A\n3. Response B\n\nNow provide your evaluation and ranking:\"\"\"\n\n\nasync def stage2_collect_rankings(\n    user_query: str,\n    stage1_results: List[Dict[str, Any]],\n    use_circuit_breaker: bool = True,\n    ranking_criteria: Optional[List[Dict[str, Any]]] = None\n) -> Tuple[List[Dict[str, Any]], Dict[str, str], List[Dict[str, Any]]]:\n    \"\"\"\n    Stage 2: Each model ranks the anonymized responses.\n\n    Args:\n        user_query: The original user query\n        stage1_results: Results from Stage 1\n        use_circuit_breaker: Whether to use circuit breaker protection\n        ranking_criteria: Optional custom ranking criteria\n\n    Returns:\n        Tuple of (rankings list, label_to_model mapping, metrics list)\n    \"\"\"\n    # Create anonymized labels for responses (Response A, Response B, etc.)\n    labels = [chr(65 + i) for i in range(len(stage1_results))]  # A, B, C, ...\n\n    # Create mapping from label to model name\n    label_to_model = {\n        f\"Response {label}\": result['model']\n        for label, result in zip(labels, stage1_results)\n    }\n\n    # Build the ranking prompt\n    responses_text = \"\\n\\n\".join([\n        f\"Response {label}:\\n{result['response']}\"\n        for label, result in zip(labels, stage1_results)\n    ])\n\n    ranking_prompt = build_ranking_prompt(user_query, responses_text, ranking_criteria)\n\n    messages = [{\"role\": \"user\", \"content\": ranking_prompt}]\n\n    # Get rankings from all council models in parallel\n    council_models = get_council_models()\n    model_params = get_model_parameters()\n\n    if use_circuit_breaker:\n        responses = await query_models_parallel_with_circuit_breaker(council_models, messages, model_params)\n    else:\n        responses = await query_models_parallel(council_models, messages, model_params)\n\n    # Format results and collect metrics\n    stage2_results = []\n    stage2_metrics = []\n\n    for model, response in responses.items():\n        if response is not None:\n            full_text = response.get('content', '')\n            metrics = response.get('metrics', {})\n\n            # Build metrics record\n            metric_record = {\n                'model_id': model,\n                'stage': 'stage2',\n                'response_time_ms': metrics.get('response_time_ms'),\n                'input_tokens': metrics.get('input_tokens'),\n                'output_tokens': metrics.get('output_tokens'),\n                'total_tokens': metrics.get('total_tokens'),\n                'error_type': metrics.get('error_type'),\n            }\n\n            if metrics.get('total_tokens'):\n                metric_record['cost_usd'] = estimate_cost(model, metrics)\n\n            stage2_metrics.append(metric_record)\n\n            if full_text:\n                parsed = parse_ranking_from_text(full_text)\n                stage2_results.append({\n                    \"model\": model,\n                    \"ranking\": full_text,\n                    \"parsed_ranking\": parsed\n                })\n\n    return stage2_results, label_to_model, stage2_metrics\n\n\ndef build_chairman_prompt(\n    user_query: str,\n    stage1_results: List[Dict[str, Any]],\n    stage2_results: List[Dict[str, Any]]\n) -> str:\n    \"\"\"Build the chairman synthesis prompt.\"\"\"\n    stage1_text = \"\\n\\n\".join([\n        f\"Model: {result['model']}\\nResponse: {result['response']}\"\n        for result in stage1_results\n    ])\n\n    stage2_text = \"\\n\\n\".join([\n        f\"Model: {result['model']}\\nRanking: {result['ranking']}\"\n        for result in stage2_results\n    ])\n\n    return f\"\"\"You are the Chairman of an LLM Council. Multiple AI models have provided responses to a user's question, and then ranked each other's responses.\n\nOriginal Question: {user_query}\n\nSTAGE 1 - Individual Responses:\n{stage1_text}\n\nSTAGE 2 - Peer Rankings:\n{stage2_text}\n\nYour task as Chairman is to synthesize all of this information into a single, comprehensive, accurate answer to the user's original question. Consider:\n- The individual responses and their insights\n- The peer rankings and what they reveal about response quality\n- Any patterns of agreement or disagreement\n\nProvide a clear, well-reasoned final answer that represents the council's collective wisdom:\"\"\"\n\n\nasync def stage3_synthesize_final(\n    user_query: str,\n    stage1_results: List[Dict[str, Any]],\n    stage2_results: List[Dict[str, Any]],\n    use_circuit_breaker: bool = True\n) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n    \"\"\"\n    Stage 3: Chairman synthesizes final response.\n\n    Args:\n        user_query: The original user query\n        stage1_results: Individual model responses from Stage 1\n        stage2_results: Rankings from Stage 2\n        use_circuit_breaker: Whether to use circuit breaker protection\n\n    Returns:\n        Tuple of (result dict, metrics dict)\n    \"\"\"\n    chairman_prompt = build_chairman_prompt(user_query, stage1_results, stage2_results)\n    messages = [{\"role\": \"user\", \"content\": chairman_prompt}]\n\n    # Query the chairman model\n    chairman_model = get_chairman_model()\n    model_params = get_model_parameters()\n\n    # Get chairman-specific parameters\n    chairman_params = {}\n    if chairman_model in model_params:\n        mp = model_params[chairman_model]\n        if \"temperature\" in mp:\n            chairman_params[\"temperature\"] = mp[\"temperature\"]\n        if \"max_tokens\" in mp:\n            chairman_params[\"max_tokens\"] = mp[\"max_tokens\"]\n\n    if use_circuit_breaker:\n        response = await query_model_with_circuit_breaker(chairman_model, messages, **chairman_params)\n    else:\n        response = await query_model(chairman_model, messages, **chairman_params)\n\n    # Build metrics\n    metrics = response.get('metrics', {}) if response else {}\n    stage3_metrics = {\n        'model_id': chairman_model,\n        'stage': 'stage3',\n        'response_time_ms': metrics.get('response_time_ms'),\n        'input_tokens': metrics.get('input_tokens'),\n        'output_tokens': metrics.get('output_tokens'),\n        'total_tokens': metrics.get('total_tokens'),\n        'error_type': metrics.get('error_type'),\n    }\n\n    if metrics.get('total_tokens'):\n        stage3_metrics['cost_usd'] = estimate_cost(chairman_model, metrics)\n\n    if response is None or not response.get('content'):\n        # Fallback if chairman fails\n        return {\n            \"model\": chairman_model,\n            \"response\": \"Error: Unable to generate final synthesis.\"\n        }, stage3_metrics\n\n    return {\n        \"model\": chairman_model,\n        \"response\": response.get('content', '')\n    }, stage3_metrics\n\n\nasync def stage3_synthesize_streaming(\n    user_query: str,\n    stage1_results: List[Dict[str, Any]],\n    stage2_results: List[Dict[str, Any]]\n) -> AsyncGenerator[Dict[str, Any], None]:\n    \"\"\"\n    Stage 3: Chairman synthesizes final response with token streaming.\n\n    Yields:\n        Dict with 'type' (token, done, error), 'model', and 'content'\n    \"\"\"\n    chairman_prompt = build_chairman_prompt(user_query, stage1_results, stage2_results)\n    messages = [{\"role\": \"user\", \"content\": chairman_prompt}]\n\n    chairman_model = get_chairman_model()\n    model_params = get_model_parameters()\n\n    # Get chairman-specific parameters for streaming\n    stream_params = {}\n    if chairman_model in model_params:\n        mp = model_params[chairman_model]\n        if \"temperature\" in mp:\n            stream_params[\"temperature\"] = mp[\"temperature\"]\n        if \"max_tokens\" in mp:\n            stream_params[\"max_tokens\"] = mp[\"max_tokens\"]\n\n    # Check circuit breaker\n    if not CircuitBreaker.can_execute(chairman_model):\n        yield {\n            \"type\": \"error\",\n            \"model\": chairman_model,\n            \"content\": \"Circuit breaker open - model temporarily unavailable\"\n        }\n        return\n\n    async for chunk in stream_model(chairman_model, messages, **stream_params):\n        yield {\n            \"type\": chunk[\"type\"],\n            \"model\": chairman_model,\n            \"content\": chunk[\"content\"]\n        }\n\n        # Record success/failure based on streaming result\n        if chunk[\"type\"] == \"done\":\n            CircuitBreaker.record_success(chairman_model)\n        elif chunk[\"type\"] == \"error\":\n            CircuitBreaker.record_failure(chairman_model)\n\n\ndef parse_ranking_from_text(ranking_text: str) -> List[str]:\n    \"\"\"\n    Parse the FINAL RANKING section from the model's response.\n\n    Args:\n        ranking_text: The full text response from the model\n\n    Returns:\n        List of response labels in ranked order\n    \"\"\"\n    import re\n\n    # Look for \"FINAL RANKING:\" section\n    if \"FINAL RANKING:\" in ranking_text:\n        # Extract everything after \"FINAL RANKING:\"\n        parts = ranking_text.split(\"FINAL RANKING:\")\n        if len(parts) >= 2:\n            ranking_section = parts[1]\n            # Try to extract numbered list format (e.g., \"1. Response A\")\n            # This pattern looks for: number, period, optional space, \"Response X\"\n            numbered_matches = re.findall(r'\\d+\\.\\s*Response [A-Z]', ranking_section)\n            if numbered_matches:\n                # Extract just the \"Response X\" part\n                return [re.search(r'Response [A-Z]', m).group() for m in numbered_matches]\n\n            # Fallback: Extract all \"Response X\" patterns in order\n            matches = re.findall(r'Response [A-Z]', ranking_section)\n            return matches\n\n    # Fallback: try to find any \"Response X\" patterns in order\n    matches = re.findall(r'Response [A-Z]', ranking_text)\n    return matches\n\n\ndef calculate_aggregate_rankings(\n    stage2_results: List[Dict[str, Any]],\n    label_to_model: Dict[str, str],\n    model_weights: Optional[Dict[str, float]] = None\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Calculate aggregate rankings across all models with optional weighted voting.\n\n    Args:\n        stage2_results: Rankings from each model\n        label_to_model: Mapping from anonymous labels to model names\n        model_weights: Optional dict mapping model_id to voting weight (default 1.0)\n\n    Returns:\n        List of dicts with model name and average rank, sorted best to worst\n    \"\"\"\n    from collections import defaultdict\n\n    # Track positions for each model with weights\n    model_positions = defaultdict(list)\n    model_weighted_positions = defaultdict(list)\n\n    for ranking in stage2_results:\n        ranking_text = ranking['ranking']\n        ranker_model = ranking.get('model', '')\n\n        # Get weight for the ranking model (default 1.0)\n        weight = 1.0\n        if model_weights and ranker_model in model_weights:\n            weight = model_weights[ranker_model]\n\n        # Parse the ranking from the structured format\n        parsed_ranking = parse_ranking_from_text(ranking_text)\n\n        for position, label in enumerate(parsed_ranking, start=1):\n            if label in label_to_model:\n                model_name = label_to_model[label]\n                model_positions[model_name].append(position)\n                model_weighted_positions[model_name].append((position, weight))\n\n    # Calculate average position for each model\n    aggregate = []\n    for model, positions in model_positions.items():\n        if positions:\n            # Calculate unweighted average\n            avg_rank = sum(positions) / len(positions)\n\n            # Calculate weighted average if weights provided\n            weighted_positions = model_weighted_positions[model]\n            total_weight = sum(w for _, w in weighted_positions)\n            weighted_avg = sum(p * w for p, w in weighted_positions) / total_weight if total_weight > 0 else avg_rank\n\n            aggregate.append({\n                \"model\": model,\n                \"average_rank\": round(weighted_avg, 2),\n                \"unweighted_rank\": round(avg_rank, 2),\n                \"rankings_count\": len(positions)\n            })\n\n    # Sort by average rank (lower is better)\n    aggregate.sort(key=lambda x: x['average_rank'])\n\n    return aggregate\n\n\ndef calculate_dissent_metrics(\n    stage2_results: List[Dict[str, Any]],\n    label_to_model: Dict[str, str]\n) -> Dict[str, Any]:\n    \"\"\"\n    Calculate disagreement/dissent metrics from Stage 2 rankings.\n\n    Args:\n        stage2_results: Rankings from each model\n        label_to_model: Mapping from anonymous labels to model names\n\n    Returns:\n        Dict with dissent metrics: agreement_score, controversies, unanimous_winner\n    \"\"\"\n    from collections import defaultdict\n\n    if len(stage2_results) < 2:\n        return {\"agreement_score\": 1.0, \"controversies\": [], \"unanimous_winner\": None}\n\n    # Track who each model ranked first and last\n    first_place_votes = defaultdict(list)  # model -> list of rankers who put them first\n    last_place_votes = defaultdict(list)   # model -> list of rankers who put them last\n    all_rankings = []  # list of (ranker, [ranked models in order])\n\n    for ranking in stage2_results:\n        ranker = ranking.get('model', 'unknown')\n        parsed = parse_ranking_from_text(ranking['ranking'])\n\n        if parsed:\n            # Convert labels to model names\n            ranked_models = [label_to_model.get(label, label) for label in parsed]\n            all_rankings.append((ranker, ranked_models))\n\n            if ranked_models:\n                first_place_votes[ranked_models[0]].append(ranker)\n                last_place_votes[ranked_models[-1]].append(ranker)\n\n    # Calculate agreement score (0-1, 1 = perfect agreement)\n    # Based on Kendall's W (coefficient of concordance) simplified\n    num_rankers = len(all_rankings)\n    if num_rankers < 2:\n        return {\"agreement_score\": 1.0, \"controversies\": [], \"unanimous_winner\": None}\n\n    # Check for unanimous first place\n    unanimous_winner = None\n    for model, voters in first_place_votes.items():\n        if len(voters) == num_rankers:\n            unanimous_winner = model\n            break\n\n    # Find controversial responses (models with both first and last place votes)\n    controversies = []\n    for model in set(first_place_votes.keys()) | set(last_place_votes.keys()):\n        first_votes = len(first_place_votes.get(model, []))\n        last_votes = len(last_place_votes.get(model, []))\n        if first_votes > 0 and last_votes > 0:\n            controversies.append({\n                \"model\": model,\n                \"first_place_votes\": first_votes,\n                \"last_place_votes\": last_votes,\n                \"controversy_score\": round((first_votes + last_votes) / num_rankers, 2)\n            })\n\n    # Sort controversies by controversy score\n    controversies.sort(key=lambda x: x['controversy_score'], reverse=True)\n\n    # Calculate simple agreement score based on position variance\n    # Lower variance = higher agreement\n    from collections import defaultdict\n    position_lists = defaultdict(list)\n    for ranker, ranked_models in all_rankings:\n        for pos, model in enumerate(ranked_models, start=1):\n            position_lists[model].append(pos)\n\n    if position_lists:\n        variances = []\n        for model, positions in position_lists.items():\n            if len(positions) > 1:\n                mean = sum(positions) / len(positions)\n                variance = sum((p - mean) ** 2 for p in positions) / len(positions)\n                variances.append(variance)\n\n        if variances:\n            avg_variance = sum(variances) / len(variances)\n            # Normalize: variance of 0 = agreement 1.0, high variance = lower agreement\n            # Max possible variance for n items is roughly (n-1)^2 / 4\n            max_variance = (len(label_to_model) - 1) ** 2 / 4 if len(label_to_model) > 1 else 1\n            agreement_score = max(0, 1 - (avg_variance / max_variance)) if max_variance > 0 else 1.0\n        else:\n            agreement_score = 1.0\n    else:\n        agreement_score = 1.0\n\n    return {\n        \"agreement_score\": round(agreement_score, 2),\n        \"controversies\": controversies,\n        \"unanimous_winner\": unanimous_winner\n    }\n\n\nasync def generate_conversation_title(user_query: str) -> str:\n    \"\"\"\n    Generate a short title for a conversation based on the first user message.\n\n    Args:\n        user_query: The first user message\n\n    Returns:\n        A short title (3-5 words)\n    \"\"\"\n    title_prompt = f\"\"\"Generate a very short title (3-5 words maximum) that summarizes the following question.\nThe title should be concise and descriptive. Do not use quotes or punctuation in the title.\n\nQuestion: {user_query}\n\nTitle:\"\"\"\n\n    messages = [{\"role\": \"user\", \"content\": title_prompt}]\n\n    # Use gemini-2.5-flash for title generation (fast and cheap)\n    response = await query_model(\"google/gemini-2.5-flash\", messages, timeout=30.0)\n\n    if response is None or response.get('content') is None:\n        # Fallback to a generic title\n        return \"New Conversation\"\n\n    title = response.get('content', 'New Conversation').strip()\n\n    # Clean up the title - remove quotes, limit length\n    title = title.strip('\"\\'')\n\n    # Truncate if too long\n    if len(title) > 50:\n        title = title[:47] + \"...\"\n\n    return title\n\n\nasync def run_full_council(\n    user_query: str,\n    ranking_criteria: Optional[List[Dict[str, Any]]] = None,\n    model_weights: Optional[Dict[str, float]] = None,\n    enable_dissent_tracking: bool = True,\n    enable_confidence: bool = False\n) -> Tuple[List, List, Dict, Dict, List]:\n    \"\"\"\n    Run the complete 3-stage council process.\n\n    Args:\n        user_query: The user's question\n        ranking_criteria: Optional custom ranking criteria for Stage 2\n        model_weights: Optional model voting weights for aggregate rankings\n        enable_dissent_tracking: Whether to calculate dissent metrics\n        enable_confidence: Whether to ask models for confidence scores\n\n    Returns:\n        Tuple of (stage1_results, stage2_results, stage3_result, metadata, all_metrics)\n    \"\"\"\n    all_metrics = []\n\n    # Stage 1: Collect individual responses\n    stage1_results, stage1_metrics = await stage1_collect_responses(\n        user_query,\n        enable_confidence=enable_confidence\n    )\n    all_metrics.extend(stage1_metrics)\n\n    # If no models responded successfully, return error\n    if not stage1_results:\n        return [], [], {\n            \"model\": \"error\",\n            \"response\": \"All models failed to respond. Please try again.\"\n        }, {}, all_metrics\n\n    # Stage 2: Collect rankings with optional custom criteria\n    stage2_results, label_to_model, stage2_metrics = await stage2_collect_rankings(\n        user_query,\n        stage1_results,\n        ranking_criteria=ranking_criteria\n    )\n    all_metrics.extend(stage2_metrics)\n\n    # Calculate aggregate rankings with optional weighted voting\n    aggregate_rankings = calculate_aggregate_rankings(\n        stage2_results,\n        label_to_model,\n        model_weights=model_weights\n    )\n\n    # Add ranking positions to stage1 metrics\n    for ranking in aggregate_rankings:\n        for metric in all_metrics:\n            if metric['model_id'] == ranking['model'] and metric['stage'] == 'stage1':\n                metric['ranking_position'] = ranking['average_rank']\n\n    # Stage 3: Synthesize final answer\n    stage3_result, stage3_metrics = await stage3_synthesize_final(\n        user_query,\n        stage1_results,\n        stage2_results\n    )\n    all_metrics.append(stage3_metrics)\n\n    # Prepare metadata\n    metadata = {\n        \"label_to_model\": label_to_model,\n        \"aggregate_rankings\": aggregate_rankings\n    }\n\n    # Calculate dissent metrics if enabled\n    if enable_dissent_tracking and len(stage2_results) >= 2:\n        dissent_metrics = calculate_dissent_metrics(stage2_results, label_to_model)\n        metadata[\"dissent\"] = dissent_metrics\n\n    return stage1_results, stage2_results, stage3_result, metadata, all_metrics\n","numLines":840,"startLine":1,"totalLines":840}}}
{"parentUuid":"f589f3cf-0784-4206-86c4-e29efce9ab91","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01MXBrzsqekoS2aAVL9kDLFE","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me examine the config and check for system prompts:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":13288,"cache_read_input_tokens":67311,"cache_creation":{"ephemeral_5m_input_tokens":13288,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWhmP6zShuKHMTo1aAPYu","type":"assistant","uuid":"f3546e73-2719-47df-af78-b1b4b7b53605","timestamp":"2026-01-02T02:31:05.384Z"}
{"parentUuid":"f3546e73-2719-47df-af78-b1b4b7b53605","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01MXBrzsqekoS2aAVL9kDLFE","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019veZW58mqGKyEjDkz653gd","name":"Read","input":{"file_path":"/Users/jeremy/Projects/LLMCouncil/backend/config.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":13288,"cache_read_input_tokens":67311,"cache_creation":{"ephemeral_5m_input_tokens":13288,"ephemeral_1h_input_tokens":0},"output_tokens":85,"service_tier":"standard"}},"requestId":"req_011CWhmP6zShuKHMTo1aAPYu","type":"assistant","uuid":"9d60cd8d-62a4-416c-b0e6-860bd6bb150d","timestamp":"2026-01-02T02:31:05.754Z"}
{"parentUuid":"9d60cd8d-62a4-416c-b0e6-860bd6bb150d","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019veZW58mqGKyEjDkz653gd","type":"tool_result","content":"     1→\"\"\"Configuration for the LLM Council.\"\"\"\n     2→\n     3→import os\n     4→from typing import List\n     5→from dotenv import load_dotenv\n     6→\n     7→load_dotenv()\n     8→\n     9→# OpenRouter API key\n    10→OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n    11→\n    12→# OpenRouter API endpoint\n    13→OPENROUTER_API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n    14→\n    15→# Data directory for storage\n    16→DATA_DIR = \"data\"\n    17→\n    18→\n    19→def get_council_models() -> List[str]:\n    20→    \"\"\"Get current council models from database or defaults.\"\"\"\n    21→    try:\n    22→        from .storage import get_council_config\n    23→        config = get_council_config()\n    24→        return config.get(\"council_models\", _DEFAULT_COUNCIL_MODELS)\n    25→    except Exception:\n    26→        return _DEFAULT_COUNCIL_MODELS\n    27→\n    28→\n    29→def get_chairman_model() -> str:\n    30→    \"\"\"Get current chairman model from database or default.\"\"\"\n    31→    try:\n    32→        from .storage import get_council_config\n    33→        config = get_council_config()\n    34→        return config.get(\"chairman_model\", _DEFAULT_CHAIRMAN_MODEL)\n    35→    except Exception:\n    36→        return _DEFAULT_CHAIRMAN_MODEL\n    37→\n    38→\n    39→def get_model_parameters() -> dict:\n    40→    \"\"\"Get per-model parameters (temperature, max_tokens) from database or empty dict.\"\"\"\n    41→    try:\n    42→        from .storage import get_council_config\n    43→        config = get_council_config()\n    44→        return config.get(\"model_parameters\", {})\n    45→    except Exception:\n    46→        return {}\n    47→\n    48→\n    49→# Default council configuration (used as fallback)\n    50→_DEFAULT_COUNCIL_MODELS = [\n    51→    \"openai/gpt-4o\",\n    52→    \"anthropic/claude-3.5-sonnet\",\n    53→    \"google/gemini-2.5-flash\",\n    54→    \"x-ai/grok-3\",\n    55→]\n    56→\n    57→_DEFAULT_CHAIRMAN_MODEL = \"google/gemini-2.5-flash\"\n    58→\n    59→# For backwards compatibility - these now call the dynamic functions\n    60→# but modules can import them directly\n    61→COUNCIL_MODELS = property(lambda self: get_council_models())\n    62→CHAIRMAN_MODEL = property(lambda self: get_chairman_model())\n    63→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"850cddc0-9ac0-4215-a69a-42b8a9805511","timestamp":"2026-01-02T02:31:06.129Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/jeremy/Projects/LLMCouncil/backend/config.py","content":"\"\"\"Configuration for the LLM Council.\"\"\"\n\nimport os\nfrom typing import List\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# OpenRouter API key\nOPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n\n# OpenRouter API endpoint\nOPENROUTER_API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n\n# Data directory for storage\nDATA_DIR = \"data\"\n\n\ndef get_council_models() -> List[str]:\n    \"\"\"Get current council models from database or defaults.\"\"\"\n    try:\n        from .storage import get_council_config\n        config = get_council_config()\n        return config.get(\"council_models\", _DEFAULT_COUNCIL_MODELS)\n    except Exception:\n        return _DEFAULT_COUNCIL_MODELS\n\n\ndef get_chairman_model() -> str:\n    \"\"\"Get current chairman model from database or default.\"\"\"\n    try:\n        from .storage import get_council_config\n        config = get_council_config()\n        return config.get(\"chairman_model\", _DEFAULT_CHAIRMAN_MODEL)\n    except Exception:\n        return _DEFAULT_CHAIRMAN_MODEL\n\n\ndef get_model_parameters() -> dict:\n    \"\"\"Get per-model parameters (temperature, max_tokens) from database or empty dict.\"\"\"\n    try:\n        from .storage import get_council_config\n        config = get_council_config()\n        return config.get(\"model_parameters\", {})\n    except Exception:\n        return {}\n\n\n# Default council configuration (used as fallback)\n_DEFAULT_COUNCIL_MODELS = [\n    \"openai/gpt-4o\",\n    \"anthropic/claude-3.5-sonnet\",\n    \"google/gemini-2.5-flash\",\n    \"x-ai/grok-3\",\n]\n\n_DEFAULT_CHAIRMAN_MODEL = \"google/gemini-2.5-flash\"\n\n# For backwards compatibility - these now call the dynamic functions\n# but modules can import them directly\nCOUNCIL_MODELS = property(lambda self: get_council_models())\nCHAIRMAN_MODEL = property(lambda self: get_chairman_model())\n","numLines":63,"startLine":1,"totalLines":63}}}
{"parentUuid":"850cddc0-9ac0-4215-a69a-42b8a9805511","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013GsDuRTs8utn8ysKkqWFd2","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check the main.py to see any additional prompts or system instructions:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":1031,"cache_read_input_tokens":80599,"cache_creation":{"ephemeral_5m_input_tokens":1031,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWhmPQRgXeXonDm3tyr5C","type":"assistant","uuid":"a87c7b03-98d2-4762-bc83-e4b96b3d5f51","timestamp":"2026-01-02T02:31:10.288Z"}
{"parentUuid":"a87c7b03-98d2-4762-bc83-e4b96b3d5f51","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_013GsDuRTs8utn8ysKkqWFd2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01CJRYJBHNnPySqTVerLKyCY","name":"Read","input":{"file_path":"/Users/jeremy/Projects/LLMCouncil/backend/main.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":1031,"cache_read_input_tokens":80599,"cache_creation":{"ephemeral_5m_input_tokens":1031,"ephemeral_1h_input_tokens":0},"output_tokens":90,"service_tier":"standard"}},"requestId":"req_011CWhmPQRgXeXonDm3tyr5C","type":"assistant","uuid":"6a41d1a7-30de-427b-8704-3dc998205537","timestamp":"2026-01-02T02:31:10.489Z"}
{"parentUuid":"6a41d1a7-30de-427b-8704-3dc998205537","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01CJRYJBHNnPySqTVerLKyCY","type":"tool_result","content":"     1→\"\"\"FastAPI backend for LLM Council.\"\"\"\n     2→\n     3→import os\n     4→from fastapi import FastAPI, HTTPException\n     5→from fastapi.middleware.cors import CORSMiddleware\n     6→from fastapi.responses import StreamingResponse\n     7→from pydantic import BaseModel\n     8→from typing import List, Dict, Any, Optional\n     9→import uuid\n    10→import json\n    11→import asyncio\n    12→\n    13→from . import storage\n    14→from .council import (\n    15→    run_full_council,\n    16→    generate_conversation_title,\n    17→    stage1_collect_responses,\n    18→    stage2_collect_rankings,\n    19→    stage3_synthesize_final,\n    20→    stage3_synthesize_streaming,\n    21→    calculate_aggregate_rankings,\n    22→    calculate_dissent_metrics\n    23→)\n    24→from .openrouter import CircuitBreaker, fetch_available_models, format_model_for_display\n    25→from .db.session import init_db, migrate_from_json\n    26→\n    27→# Initialize database on module load\n    28→init_db()\n    29→\n    30→# Migrate existing JSON conversations if any\n    31→migrate_from_json()\n    32→\n    33→app = FastAPI(title=\"LLM Council API\", version=\"1.1.0\")\n    34→\n    35→# CORS origins - allow localhost for dev and configured frontend URL for production\n    36→cors_origins = [\n    37→    \"http://localhost:5173\",\n    38→    \"http://localhost:3000\",\n    39→    \"https://llmcouncil.digital\",\n    40→    \"https://www.llmcouncil.digital\",\n    41→]\n    42→\n    43→# Add production frontend URL if configured\n    44→frontend_url = os.environ.get(\"FRONTEND_URL\")\n    45→if frontend_url:\n    46→    cors_origins.append(frontend_url)\n    47→    # Also allow without trailing slash\n    48→    cors_origins.append(frontend_url.rstrip(\"/\"))\n    49→\n    50→app.add_middleware(\n    51→    CORSMiddleware,\n    52→    allow_origins=cors_origins,\n    53→    allow_credentials=True,\n    54→    allow_methods=[\"*\"],\n    55→    allow_headers=[\"*\"],\n    56→)\n    57→\n    58→\n    59→# Request/Response Models\n    60→\n    61→class CreateConversationRequest(BaseModel):\n    62→    \"\"\"Request to create a new conversation.\"\"\"\n    63→    pass\n    64→\n    65→\n    66→class SendMessageRequest(BaseModel):\n    67→    \"\"\"Request to send a message in a conversation.\"\"\"\n    68→    content: str\n    69→\n    70→\n    71→class ConversationMetadata(BaseModel):\n    72→    \"\"\"Conversation metadata for list view.\"\"\"\n    73→    id: str\n    74→    created_at: str\n    75→    title: str\n    76→    message_count: int\n    77→\n    78→\n    79→class Conversation(BaseModel):\n    80→    \"\"\"Full conversation with all messages.\"\"\"\n    81→    id: str\n    82→    created_at: str\n    83→    title: str\n    84→    messages: List[Dict[str, Any]]\n    85→\n    86→\n    87→class RankingCriterion(BaseModel):\n    88→    \"\"\"A single ranking criterion for Stage 2 evaluation.\"\"\"\n    89→    id: str\n    90→    name: str\n    91→    description: str = \"\"\n    92→    weight: float = 1.0\n    93→    enabled: bool = True\n    94→\n    95→\n    96→class CouncilConfigRequest(BaseModel):\n    97→    \"\"\"Request to update council configuration.\"\"\"\n    98→    council_models: Optional[List[str]] = None\n    99→    chairman_model: Optional[str] = None\n   100→    theme: Optional[str] = None\n   101→    # Phase 4: Advanced Deliberation\n   102→    ranking_criteria: Optional[List[Dict[str, Any]]] = None\n   103→    model_weights: Optional[Dict[str, float]] = None\n   104→    enable_confidence: Optional[bool] = None\n   105→    enable_dissent_tracking: Optional[bool] = None\n   106→    # Phase 5: Model Management\n   107→    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n   108→\n   109→\n   110→class CouncilConfigResponse(BaseModel):\n   111→    \"\"\"Council configuration response.\"\"\"\n   112→    council_models: List[str]\n   113→    chairman_model: str\n   114→    theme: str\n   115→    # Phase 4: Advanced Deliberation\n   116→    ranking_criteria: List[Dict[str, Any]] = []\n   117→    model_weights: Dict[str, float] = {}\n   118→    enable_confidence: bool = False\n   119→    enable_dissent_tracking: bool = True\n   120→    # Phase 5: Model Management\n   121→    model_parameters: Dict[str, Dict[str, Any]] = {}\n   122→\n   123→\n   124→class ExportRequest(BaseModel):\n   125→    \"\"\"Request to export a conversation.\"\"\"\n   126→    format: str = \"markdown\"  # markdown, json\n   127→\n   128→\n   129→class PresetRequest(BaseModel):\n   130→    \"\"\"Request to create or update a preset.\"\"\"\n   131→    name: str\n   132→    description: Optional[str] = None\n   133→    council_models: List[str]\n   134→    chairman_model: str\n   135→    model_weights: Optional[Dict[str, float]] = None\n   136→    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n   137→    ranking_criteria: Optional[List[Dict[str, Any]]] = None\n   138→\n   139→\n   140→class PresetUpdateRequest(BaseModel):\n   141→    \"\"\"Request to update a preset (all fields optional).\"\"\"\n   142→    name: Optional[str] = None\n   143→    description: Optional[str] = None\n   144→    council_models: Optional[List[str]] = None\n   145→    chairman_model: Optional[str] = None\n   146→    model_weights: Optional[Dict[str, float]] = None\n   147→    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n   148→    ranking_criteria: Optional[List[Dict[str, Any]]] = None\n   149→\n   150→\n   151→# Health Check\n   152→\n   153→@app.get(\"/\")\n   154→async def root():\n   155→    \"\"\"Health check endpoint.\"\"\"\n   156→    return {\"status\": \"ok\", \"service\": \"LLM Council API\", \"version\": \"1.1.0\"}\n   157→\n   158→\n   159→# Conversation Endpoints\n   160→\n   161→@app.get(\"/api/conversations\", response_model=List[ConversationMetadata])\n   162→async def list_conversations():\n   163→    \"\"\"List all conversations (metadata only).\"\"\"\n   164→    return storage.list_conversations()\n   165→\n   166→\n   167→@app.post(\"/api/conversations\", response_model=Conversation)\n   168→async def create_conversation(request: CreateConversationRequest):\n   169→    \"\"\"Create a new conversation.\"\"\"\n   170→    conversation_id = str(uuid.uuid4())\n   171→    conversation = storage.create_conversation(conversation_id)\n   172→    return conversation\n   173→\n   174→\n   175→@app.get(\"/api/conversations/{conversation_id}\", response_model=Conversation)\n   176→async def get_conversation(conversation_id: str):\n   177→    \"\"\"Get a specific conversation with all its messages.\"\"\"\n   178→    conversation = storage.get_conversation(conversation_id)\n   179→    if conversation is None:\n   180→        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n   181→    return conversation\n   182→\n   183→\n   184→@app.delete(\"/api/conversations/{conversation_id}\")\n   185→async def delete_conversation(conversation_id: str):\n   186→    \"\"\"Delete a conversation.\"\"\"\n   187→    deleted = storage.delete_conversation(conversation_id)\n   188→    if not deleted:\n   189→        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n   190→    return {\"status\": \"deleted\", \"id\": conversation_id}\n   191→\n   192→\n   193→@app.post(\"/api/conversations/{conversation_id}/message\")\n   194→async def send_message(conversation_id: str, request: SendMessageRequest):\n   195→    \"\"\"\n   196→    Send a message and run the 3-stage council process.\n   197→    Returns the complete response with all stages.\n   198→    \"\"\"\n   199→    # Check if conversation exists\n   200→    conversation = storage.get_conversation(conversation_id)\n   201→    if conversation is None:\n   202→        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n   203→\n   204→    # Check if this is the first message\n   205→    is_first_message = len(conversation[\"messages\"]) == 0\n   206→\n   207→    # Add user message\n   208→    storage.add_user_message(conversation_id, request.content)\n   209→\n   210→    # If this is the first message, generate a title\n   211→    if is_first_message:\n   212→        title = await generate_conversation_title(request.content)\n   213→        storage.update_conversation_title(conversation_id, title)\n   214→\n   215→    # Load council config for Phase 4 features\n   216→    config = storage.get_council_config()\n   217→\n   218→    # Run the 3-stage council process with Phase 4 settings\n   219→    stage1_results, stage2_results, stage3_result, metadata, all_metrics = await run_full_council(\n   220→        request.content,\n   221→        ranking_criteria=config.get(\"ranking_criteria\"),\n   222→        model_weights=config.get(\"model_weights\"),\n   223→        enable_dissent_tracking=config.get(\"enable_dissent_tracking\", True),\n   224→        enable_confidence=config.get(\"enable_confidence\", False)\n   225→    )\n   226→\n   227→    # Add assistant message with all stages\n   228→    storage.add_assistant_message(\n   229→        conversation_id,\n   230→        stage1_results,\n   231→        stage2_results,\n   232→        stage3_result,\n   233→        metadata\n   234→    )\n   235→\n   236→    # Save metrics\n   237→    message_id = storage.get_last_message_id(conversation_id)\n   238→    if message_id and all_metrics:\n   239→        storage.save_model_metrics(message_id, all_metrics)\n   240→\n   241→    # Return the complete response with metadata\n   242→    return {\n   243→        \"stage1\": stage1_results,\n   244→        \"stage2\": stage2_results,\n   245→        \"stage3\": stage3_result,\n   246→        \"metadata\": metadata\n   247→    }\n   248→\n   249→\n   250→def get_conversation_history(conversation: Dict[str, Any]) -> List[Dict[str, str]]:\n   251→    \"\"\"\n   252→    Extract conversation history for multi-turn context.\n   253→\n   254→    Returns list of {role, content} dicts suitable for LLM API.\n   255→    \"\"\"\n   256→    history = []\n   257→    for msg in conversation.get(\"messages\", []):\n   258→        if msg[\"role\"] == \"user\":\n   259→            history.append({\n   260→                \"role\": \"user\",\n   261→                \"content\": msg.get(\"content\", \"\")\n   262→            })\n   263→        elif msg[\"role\"] == \"assistant\":\n   264→            # Use the stage3 synthesis as the assistant response\n   265→            if msg.get(\"stage3\"):\n   266→                history.append({\n   267→                    \"role\": \"assistant\",\n   268→                    \"content\": msg[\"stage3\"].get(\"response\", \"\")\n   269→                })\n   270→    return history\n   271→\n   272→\n   273→@app.post(\"/api/conversations/{conversation_id}/message/stream\")\n   274→async def send_message_stream(conversation_id: str, request: SendMessageRequest):\n   275→    \"\"\"\n   276→    Send a message and stream the 3-stage council process.\n   277→    Returns Server-Sent Events as each stage completes.\n   278→    Supports multi-turn conversation context.\n   279→    \"\"\"\n   280→    # Check if conversation exists\n   281→    conversation = storage.get_conversation(conversation_id)\n   282→    if conversation is None:\n   283→        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n   284→\n   285→    # Check if this is the first message\n   286→    is_first_message = len(conversation[\"messages\"]) == 0\n   287→\n   288→    # Get conversation history for multi-turn context\n   289→    conversation_history = get_conversation_history(conversation)\n   290→\n   291→    # Load council config for Phase 4 features\n   292→    config = storage.get_council_config()\n   293→    ranking_criteria = config.get(\"ranking_criteria\")\n   294→    model_weights = config.get(\"model_weights\")\n   295→    enable_dissent_tracking = config.get(\"enable_dissent_tracking\", True)\n   296→    enable_confidence = config.get(\"enable_confidence\", False)\n   297→\n   298→    async def event_generator():\n   299→        try:\n   300→            all_metrics = []\n   301→\n   302→            # Add user message\n   303→            storage.add_user_message(conversation_id, request.content)\n   304→\n   305→            # Start title generation in parallel (don't await yet)\n   306→            title_task = None\n   307→            if is_first_message:\n   308→                title_task = asyncio.create_task(generate_conversation_title(request.content))\n   309→\n   310→            # Stage 1: Collect responses (with conversation history)\n   311→            yield f\"data: {json.dumps({'type': 'stage1_start'})}\\n\\n\"\n   312→            stage1_results, stage1_metrics = await stage1_collect_responses(\n   313→                request.content,\n   314→                conversation_history=conversation_history,\n   315→                enable_confidence=enable_confidence\n   316→            )\n   317→            all_metrics.extend(stage1_metrics)\n   318→            yield f\"data: {json.dumps({'type': 'stage1_complete', 'data': stage1_results})}\\n\\n\"\n   319→\n   320→            # Stage 2: Collect rankings with Phase 4 custom criteria\n   321→            yield f\"data: {json.dumps({'type': 'stage2_start'})}\\n\\n\"\n   322→            stage2_results, label_to_model, stage2_metrics = await stage2_collect_rankings(\n   323→                request.content,\n   324→                stage1_results,\n   325→                ranking_criteria=ranking_criteria\n   326→            )\n   327→            all_metrics.extend(stage2_metrics)\n   328→\n   329→            # Calculate aggregate rankings with Phase 4 weighted voting\n   330→            aggregate_rankings = calculate_aggregate_rankings(\n   331→                stage2_results,\n   332→                label_to_model,\n   333→                model_weights=model_weights\n   334→            )\n   335→\n   336→            # Add ranking positions to stage1 metrics\n   337→            for ranking in aggregate_rankings:\n   338→                for metric in all_metrics:\n   339→                    if metric['model_id'] == ranking['model'] and metric.get('stage') == 'stage1':\n   340→                        metric['ranking_position'] = ranking['average_rank']\n   341→\n   342→            # Build metadata with optional dissent tracking\n   343→            stage2_metadata = {\n   344→                'label_to_model': label_to_model,\n   345→                'aggregate_rankings': aggregate_rankings\n   346→            }\n   347→\n   348→            if enable_dissent_tracking and len(stage2_results) >= 2:\n   349→                dissent_metrics = calculate_dissent_metrics(stage2_results, label_to_model)\n   350→                stage2_metadata['dissent'] = dissent_metrics\n   351→\n   352→            yield f\"data: {json.dumps({'type': 'stage2_complete', 'data': stage2_results, 'metadata': stage2_metadata})}\\n\\n\"\n   353→\n   354→            # Stage 3: Synthesize final answer\n   355→            yield f\"data: {json.dumps({'type': 'stage3_start'})}\\n\\n\"\n   356→            stage3_result, stage3_metrics = await stage3_synthesize_final(request.content, stage1_results, stage2_results)\n   357→            all_metrics.append(stage3_metrics)\n   358→            yield f\"data: {json.dumps({'type': 'stage3_complete', 'data': stage3_result})}\\n\\n\"\n   359→\n   360→            # Wait for title generation if it was started\n   361→            if title_task:\n   362→                title = await title_task\n   363→                storage.update_conversation_title(conversation_id, title)\n   364→                yield f\"data: {json.dumps({'type': 'title_complete', 'data': {'title': title}})}\\n\\n\"\n   365→\n   366→            # Save complete assistant message with metadata\n   367→            metadata = {\n   368→                \"label_to_model\": label_to_model,\n   369→                \"aggregate_rankings\": aggregate_rankings\n   370→            }\n   371→            if enable_dissent_tracking and len(stage2_results) >= 2:\n   372→                metadata[\"dissent\"] = stage2_metadata.get(\"dissent\")\n   373→\n   374→            storage.add_assistant_message(\n   375→                conversation_id,\n   376→                stage1_results,\n   377→                stage2_results,\n   378→                stage3_result,\n   379→                metadata\n   380→            )\n   381→\n   382→            # Save metrics\n   383→            message_id = storage.get_last_message_id(conversation_id)\n   384→            if message_id and all_metrics:\n   385→                storage.save_model_metrics(message_id, all_metrics)\n   386→\n   387→            # Send completion event\n   388→            yield f\"data: {json.dumps({'type': 'complete'})}\\n\\n\"\n   389→\n   390→        except Exception as e:\n   391→            # Send error event\n   392→            yield f\"data: {json.dumps({'type': 'error', 'message': str(e)})}\\n\\n\"\n   393→\n   394→    return StreamingResponse(\n   395→        event_generator(),\n   396→        media_type=\"text/event-stream\",\n   397→        headers={\n   398→            \"Cache-Control\": \"no-cache\",\n   399→            \"Connection\": \"keep-alive\",\n   400→        }\n   401→    )\n   402→\n   403→\n   404→@app.post(\"/api/conversations/{conversation_id}/message/stream-tokens\")\n   405→async def send_message_stream_tokens(conversation_id: str, request: SendMessageRequest):\n   406→    \"\"\"\n   407→    Send a message with full token-by-token streaming for Stage 3.\n   408→    Returns Server-Sent Events including individual tokens during synthesis.\n   409→    \"\"\"\n   410→    # Check if conversation exists\n   411→    conversation = storage.get_conversation(conversation_id)\n   412→    if conversation is None:\n   413→        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n   414→\n   415→    is_first_message = len(conversation[\"messages\"]) == 0\n   416→    conversation_history = get_conversation_history(conversation)\n   417→\n   418→    # Load council config for Phase 4 features\n   419→    config = storage.get_council_config()\n   420→    ranking_criteria = config.get(\"ranking_criteria\")\n   421→    model_weights = config.get(\"model_weights\")\n   422→    enable_dissent_tracking = config.get(\"enable_dissent_tracking\", True)\n   423→    enable_confidence = config.get(\"enable_confidence\", False)\n   424→\n   425→    async def event_generator():\n   426→        try:\n   427→            all_metrics = []\n   428→            storage.add_user_message(conversation_id, request.content)\n   429→\n   430→            title_task = None\n   431→            if is_first_message:\n   432→                title_task = asyncio.create_task(generate_conversation_title(request.content))\n   433→\n   434→            # Stage 1\n   435→            yield f\"data: {json.dumps({'type': 'stage1_start'})}\\n\\n\"\n   436→            stage1_results, stage1_metrics = await stage1_collect_responses(\n   437→                request.content,\n   438→                conversation_history=conversation_history,\n   439→                enable_confidence=enable_confidence\n   440→            )\n   441→            all_metrics.extend(stage1_metrics)\n   442→            yield f\"data: {json.dumps({'type': 'stage1_complete', 'data': stage1_results})}\\n\\n\"\n   443→\n   444→            # Stage 2 with Phase 4 custom criteria\n   445→            yield f\"data: {json.dumps({'type': 'stage2_start'})}\\n\\n\"\n   446→            stage2_results, label_to_model, stage2_metrics = await stage2_collect_rankings(\n   447→                request.content,\n   448→                stage1_results,\n   449→                ranking_criteria=ranking_criteria\n   450→            )\n   451→            all_metrics.extend(stage2_metrics)\n   452→\n   453→            # Calculate aggregate rankings with Phase 4 weighted voting\n   454→            aggregate_rankings = calculate_aggregate_rankings(\n   455→                stage2_results,\n   456→                label_to_model,\n   457→                model_weights=model_weights\n   458→            )\n   459→\n   460→            # Add ranking positions to stage1 metrics\n   461→            for ranking in aggregate_rankings:\n   462→                for metric in all_metrics:\n   463→                    if metric['model_id'] == ranking['model'] and metric.get('stage') == 'stage1':\n   464→                        metric['ranking_position'] = ranking['average_rank']\n   465→\n   466→            # Build metadata with optional dissent tracking\n   467→            stage2_metadata = {\n   468→                'label_to_model': label_to_model,\n   469→                'aggregate_rankings': aggregate_rankings\n   470→            }\n   471→\n   472→            if enable_dissent_tracking and len(stage2_results) >= 2:\n   473→                dissent_metrics = calculate_dissent_metrics(stage2_results, label_to_model)\n   474→                stage2_metadata['dissent'] = dissent_metrics\n   475→\n   476→            yield f\"data: {json.dumps({'type': 'stage2_complete', 'data': stage2_results, 'metadata': stage2_metadata})}\\n\\n\"\n   477→\n   478→            # Stage 3 with token streaming\n   479→            yield f\"data: {json.dumps({'type': 'stage3_start'})}\\n\\n\"\n   480→\n   481→            full_response = \"\"\n   482→            chairman_model = \"\"\n   483→\n   484→            async for chunk in stage3_synthesize_streaming(\n   485→                request.content, stage1_results, stage2_results\n   486→            ):\n   487→                chairman_model = chunk.get(\"model\", \"\")\n   488→\n   489→                if chunk[\"type\"] == \"token\":\n   490→                    full_response += chunk[\"content\"]\n   491→                    yield f\"data: {json.dumps({'type': 'stage3_token', 'data': {'content': chunk['content']}})}\\n\\n\"\n   492→                elif chunk[\"type\"] == \"done\":\n   493→                    full_response = chunk[\"content\"]\n   494→                elif chunk[\"type\"] == \"error\":\n   495→                    yield f\"data: {json.dumps({'type': 'error', 'message': chunk['content']})}\\n\\n\"\n   496→                    return\n   497→\n   498→            stage3_result = {\n   499→                \"model\": chairman_model,\n   500→                \"response\": full_response\n   501→            }\n   502→            yield f\"data: {json.dumps({'type': 'stage3_complete', 'data': stage3_result})}\\n\\n\"\n   503→\n   504→            # Title generation\n   505→            if title_task:\n   506→                title = await title_task\n   507→                storage.update_conversation_title(conversation_id, title)\n   508→                yield f\"data: {json.dumps({'type': 'title_complete', 'data': {'title': title}})}\\n\\n\"\n   509→\n   510→            # Save message with Phase 4 metadata\n   511→            metadata = {\n   512→                \"label_to_model\": label_to_model,\n   513→                \"aggregate_rankings\": aggregate_rankings\n   514→            }\n   515→            if enable_dissent_tracking and len(stage2_results) >= 2:\n   516→                metadata[\"dissent\"] = stage2_metadata.get(\"dissent\")\n   517→\n   518→            storage.add_assistant_message(\n   519→                conversation_id,\n   520→                stage1_results,\n   521→                stage2_results,\n   522→                stage3_result,\n   523→                metadata\n   524→            )\n   525→\n   526→            # Save metrics (stage3 metrics from streaming are minimal)\n   527→            stage3_metrics = {\n   528→                'model_id': chairman_model,\n   529→                'stage': 'stage3',\n   530→            }\n   531→            all_metrics.append(stage3_metrics)\n   532→\n   533→            message_id = storage.get_last_message_id(conversation_id)\n   534→            if message_id and all_metrics:\n   535→                storage.save_model_metrics(message_id, all_metrics)\n   536→\n   537→            yield f\"data: {json.dumps({'type': 'complete'})}\\n\\n\"\n   538→\n   539→        except Exception as e:\n   540→            yield f\"data: {json.dumps({'type': 'error', 'message': str(e)})}\\n\\n\"\n   541→\n   542→    return StreamingResponse(\n   543→        event_generator(),\n   544→        media_type=\"text/event-stream\",\n   545→        headers={\n   546→            \"Cache-Control\": \"no-cache\",\n   547→            \"Connection\": \"keep-alive\",\n   548→        }\n   549→    )\n   550→\n   551→\n   552→@app.get(\"/api/circuit-breaker/status\")\n   553→async def get_circuit_breaker_status():\n   554→    \"\"\"Get circuit breaker status for all models.\"\"\"\n   555→    from .config import get_council_models, get_chairman_model\n   556→\n   557→    models = get_council_models() + [get_chairman_model()]\n   558→    status = {}\n   559→\n   560→    for model in set(models):\n   561→        status[model] = CircuitBreaker.get_status(model)\n   562→\n   563→    return {\"circuit_breaker_status\": status}\n   564→\n   565→\n   566→# Model Discovery Endpoints\n   567→\n   568→@app.get(\"/api/models\")\n   569→async def get_available_models(\n   570→    search: Optional[str] = None,\n   571→    refresh: bool = False\n   572→):\n   573→    \"\"\"\n   574→    Get available models from OpenRouter.\n   575→\n   576→    Args:\n   577→        search: Optional search term to filter models by name or id\n   578→        refresh: Force refresh from API instead of using cache\n   579→\n   580→    Returns:\n   581→        List of available models with metadata\n   582→    \"\"\"\n   583→    raw_models = await fetch_available_models(force_refresh=refresh)\n   584→\n   585→    # Format models for display\n   586→    models = [format_model_for_display(m) for m in raw_models]\n   587→\n   588→    # Filter by search term if provided\n   589→    if search:\n   590→        search_lower = search.lower()\n   591→        models = [\n   592→            m for m in models\n   593→            if search_lower in m[\"id\"].lower() or search_lower in m[\"name\"].lower()\n   594→        ]\n   595→\n   596→    # Sort by name\n   597→    models.sort(key=lambda m: m[\"name\"].lower())\n   598→\n   599→    return {\"models\": models, \"count\": len(models)}\n   600→\n   601→\n   602→@app.get(\"/api/models/popular\")\n   603→async def get_popular_models():\n   604→    \"\"\"Get a curated list of popular models for quick selection.\"\"\"\n   605→    popular_ids = [\n   606→        \"openai/gpt-4o\",\n   607→        \"openai/gpt-4o-mini\",\n   608→        \"openai/o1\",\n   609→        \"openai/o1-mini\",\n   610→        \"anthropic/claude-3.5-sonnet\",\n   611→        \"anthropic/claude-3.5-haiku\",\n   612→        \"anthropic/claude-3-opus\",\n   613→        \"google/gemini-2.5-flash\",\n   614→        \"google/gemini-2.5-pro\",\n   615→        \"meta-llama/llama-3.3-70b-instruct\",\n   616→        \"deepseek/deepseek-chat\",\n   617→        \"deepseek/deepseek-r1\",\n   618→        \"x-ai/grok-3\",\n   619→        \"x-ai/grok-3-mini\",\n   620→        \"mistralai/mistral-large\",\n   621→        \"qwen/qwen-2.5-72b-instruct\",\n   622→    ]\n   623→\n   624→    raw_models = await fetch_available_models()\n   625→\n   626→    # Find matching models\n   627→    popular = []\n   628→    model_map = {m.get(\"id\"): m for m in raw_models}\n   629→\n   630→    for model_id in popular_ids:\n   631→        if model_id in model_map:\n   632→            popular.append(format_model_for_display(model_map[model_id]))\n   633→\n   634→    return {\"models\": popular}\n   635→\n   636→\n   637→# Settings Endpoints\n   638→\n   639→@app.get(\"/api/settings/council\", response_model=CouncilConfigResponse)\n   640→async def get_council_config():\n   641→    \"\"\"Get the current council configuration.\"\"\"\n   642→    config = storage.get_council_config()\n   643→    return CouncilConfigResponse(\n   644→        council_models=config.get(\"council_models\", []),\n   645→        chairman_model=config.get(\"chairman_model\", \"\"),\n   646→        theme=config.get(\"theme\", \"light\"),\n   647→        ranking_criteria=config.get(\"ranking_criteria\", []),\n   648→        model_weights=config.get(\"model_weights\", {}),\n   649→        model_parameters=config.get(\"model_parameters\", {}),\n   650→        enable_confidence=config.get(\"enable_confidence\", False),\n   651→        enable_dissent_tracking=config.get(\"enable_dissent_tracking\", True)\n   652→    )\n   653→\n   654→\n   655→@app.put(\"/api/settings/council\", response_model=CouncilConfigResponse)\n   656→async def update_council_config(request: CouncilConfigRequest):\n   657→    \"\"\"Update the council configuration.\"\"\"\n   658→    config = storage.update_council_config(\n   659→        council_models=request.council_models,\n   660→        chairman_model=request.chairman_model,\n   661→        theme=request.theme,\n   662→        ranking_criteria=request.ranking_criteria,\n   663→        model_weights=request.model_weights,\n   664→        model_parameters=request.model_parameters,\n   665→        enable_confidence=request.enable_confidence,\n   666→        enable_dissent_tracking=request.enable_dissent_tracking\n   667→    )\n   668→    return CouncilConfigResponse(\n   669→        council_models=config.get(\"council_models\", []),\n   670→        chairman_model=config.get(\"chairman_model\", \"\"),\n   671→        theme=config.get(\"theme\", \"light\"),\n   672→        ranking_criteria=config.get(\"ranking_criteria\", []),\n   673→        model_weights=config.get(\"model_weights\", {}),\n   674→        model_parameters=config.get(\"model_parameters\", {}),\n   675→        enable_confidence=config.get(\"enable_confidence\", False),\n   676→        enable_dissent_tracking=config.get(\"enable_dissent_tracking\", True)\n   677→    )\n   678→\n   679→\n   680→# Preset Endpoints\n   681→\n   682→@app.get(\"/api/presets\")\n   683→async def list_presets():\n   684→    \"\"\"List all presets.\"\"\"\n   685→    return storage.list_presets()\n   686→\n   687→\n   688→@app.get(\"/api/presets/default\")\n   689→async def get_default_preset():\n   690→    \"\"\"Get the currently active preset.\"\"\"\n   691→    preset = storage.get_default_preset()\n   692→    if preset is None:\n   693→        return {\"preset\": None}\n   694→    return {\"preset\": preset}\n   695→\n   696→\n   697→@app.get(\"/api/presets/{preset_id}\")\n   698→async def get_preset(preset_id: int):\n   699→    \"\"\"Get a specific preset.\"\"\"\n   700→    preset = storage.get_preset(preset_id)\n   701→    if preset is None:\n   702→        raise HTTPException(status_code=404, detail=\"Preset not found\")\n   703→    return preset\n   704→\n   705→\n   706→@app.post(\"/api/presets\")\n   707→async def create_preset(request: PresetRequest):\n   708→    \"\"\"Create a new preset.\"\"\"\n   709→    preset = storage.create_preset(\n   710→        name=request.name,\n   711→        description=request.description,\n   712→        council_models=request.council_models,\n   713→        chairman_model=request.chairman_model,\n   714→        model_weights=request.model_weights,\n   715→        model_parameters=request.model_parameters,\n   716→        ranking_criteria=request.ranking_criteria\n   717→    )\n   718→    return preset\n   719→\n   720→\n   721→@app.put(\"/api/presets/{preset_id}\")\n   722→async def update_preset(preset_id: int, request: PresetUpdateRequest):\n   723→    \"\"\"Update an existing preset.\"\"\"\n   724→    preset = storage.update_preset(\n   725→        preset_id=preset_id,\n   726→        name=request.name,\n   727→        description=request.description,\n   728→        council_models=request.council_models,\n   729→        chairman_model=request.chairman_model,\n   730→        model_weights=request.model_weights,\n   731→        model_parameters=request.model_parameters,\n   732→        ranking_criteria=request.ranking_criteria\n   733→    )\n   734→    if preset is None:\n   735→        raise HTTPException(status_code=404, detail=\"Preset not found\")\n   736→    return preset\n   737→\n   738→\n   739→@app.delete(\"/api/presets/{preset_id}\")\n   740→async def delete_preset(preset_id: int):\n   741→    \"\"\"Delete a preset.\"\"\"\n   742→    deleted = storage.delete_preset(preset_id)\n   743→    if not deleted:\n   744→        raise HTTPException(status_code=404, detail=\"Preset not found\")\n   745→    return {\"status\": \"deleted\", \"id\": preset_id}\n   746→\n   747→\n   748→@app.post(\"/api/presets/{preset_id}/apply\")\n   749→async def apply_preset(preset_id: int):\n   750→    \"\"\"Apply a preset to the current council configuration.\"\"\"\n   751→    config = storage.apply_preset(preset_id)\n   752→    if config is None:\n   753→        raise HTTPException(status_code=404, detail=\"Preset not found\")\n   754→    return {\"status\": \"applied\", \"config\": config}\n   755→\n   756→\n   757→@app.post(\"/api/presets/save-current\")\n   758→async def save_current_as_preset(name: str, description: Optional[str] = None):\n   759→    \"\"\"Save the current council configuration as a new preset.\"\"\"\n   760→    config = storage.get_council_config()\n   761→    preset = storage.create_preset(\n   762→        name=name,\n   763→        description=description,\n   764→        council_models=config.get(\"council_models\", []),\n   765→        chairman_model=config.get(\"chairman_model\", \"\"),\n   766→        model_weights=config.get(\"model_weights\"),\n   767→        model_parameters=config.get(\"model_parameters\"),\n   768→        ranking_criteria=config.get(\"ranking_criteria\")\n   769→    )\n   770→    return preset\n   771→\n   772→\n   773→# Export Endpoints\n   774→\n   775→@app.post(\"/api/conversations/{conversation_id}/export\")\n   776→async def export_conversation(conversation_id: str, request: ExportRequest):\n   777→    \"\"\"Export a conversation to the specified format.\"\"\"\n   778→    conversation = storage.get_conversation(conversation_id)\n   779→    if conversation is None:\n   780→        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n   781→\n   782→    if request.format == \"json\":\n   783→        return conversation\n   784→\n   785→    elif request.format == \"markdown\":\n   786→        md = generate_markdown_export(conversation)\n   787→        return StreamingResponse(\n   788→            iter([md]),\n   789→            media_type=\"text/markdown\",\n   790→            headers={\n   791→                \"Content-Disposition\": f\"attachment; filename={conversation_id}.md\"\n   792→            }\n   793→        )\n   794→\n   795→    else:\n   796→        raise HTTPException(status_code=400, detail=f\"Unsupported format: {request.format}\")\n   797→\n   798→\n   799→def generate_markdown_export(conversation: Dict[str, Any]) -> str:\n   800→    \"\"\"Generate markdown export of a conversation.\"\"\"\n   801→    lines = [\n   802→        f\"# {conversation['title']}\",\n   803→        f\"\",\n   804→        f\"*Exported from LLM Council*\",\n   805→        f\"*Created: {conversation['created_at']}*\",\n   806→        f\"\",\n   807→        \"---\",\n   808→        \"\"\n   809→    ]\n   810→\n   811→    for msg in conversation[\"messages\"]:\n   812→        if msg[\"role\"] == \"user\":\n   813→            lines.extend([\n   814→                \"## User\",\n   815→                \"\",\n   816→                msg[\"content\"],\n   817→                \"\",\n   818→                \"---\",\n   819→                \"\"\n   820→            ])\n   821→        else:\n   822→            lines.extend([\n   823→                \"## LLM Council Response\",\n   824→                \"\",\n   825→                \"### Stage 1: Individual Responses\",\n   826→                \"\"\n   827→            ])\n   828→\n   829→            if msg.get(\"stage1\"):\n   830→                for resp in msg[\"stage1\"]:\n   831→                    lines.extend([\n   832→                        f\"#### {resp['model']}\",\n   833→                        \"\",\n   834→                        resp[\"response\"],\n   835→                        \"\"\n   836→                    ])\n   837→\n   838→            lines.extend([\n   839→                \"### Stage 2: Peer Rankings\",\n   840→                \"\"\n   841→            ])\n   842→\n   843→            if msg.get(\"stage2\"):\n   844→                for ranking in msg[\"stage2\"]:\n   845→                    lines.extend([\n   846→                        f\"#### {ranking['model']}\",\n   847→                        \"\",\n   848→                        ranking[\"ranking\"],\n   849→                        \"\"\n   850→                    ])\n   851→\n   852→            lines.extend([\n   853→                \"### Stage 3: Final Synthesis\",\n   854→                \"\"\n   855→            ])\n   856→\n   857→            if msg.get(\"stage3\"):\n   858→                lines.extend([\n   859→                    f\"*Chairman: {msg['stage3']['model']}*\",\n   860→                    \"\",\n   861→                    msg[\"stage3\"][\"response\"],\n   862→                    \"\"\n   863→                ])\n   864→\n   865→            lines.extend([\n   866→                \"---\",\n   867→                \"\"\n   868→            ])\n   869→\n   870→    return \"\\n\".join(lines)\n   871→\n   872→\n   873→# Analytics Endpoints\n   874→\n   875→@app.get(\"/api/analytics/summary\")\n   876→async def get_analytics_summary():\n   877→    \"\"\"Get aggregate analytics summary across all conversations.\"\"\"\n   878→    return storage.get_analytics_summary()\n   879→\n   880→\n   881→@app.get(\"/api/analytics/models/{model_id:path}\")\n   882→async def get_model_analytics(model_id: str):\n   883→    \"\"\"Get detailed analytics for a specific model.\"\"\"\n   884→    return storage.get_model_analytics(model_id)\n   885→\n   886→\n   887→@app.get(\"/api/analytics/recent\")\n   888→async def get_recent_metrics(limit: int = 100):\n   889→    \"\"\"Get recent metrics for timeline display.\"\"\"\n   890→    return storage.get_recent_metrics(limit)\n   891→\n   892→\n   893→if __name__ == \"__main__\":\n   894→    import uvicorn\n   895→    port = int(os.environ.get(\"PORT\", 8001))\n   896→    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n   897→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"30f2ec4e-db00-40af-a7c3-4ca357460fa5","timestamp":"2026-01-02T02:31:11.032Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/jeremy/Projects/LLMCouncil/backend/main.py","content":"\"\"\"FastAPI backend for LLM Council.\"\"\"\n\nimport os\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any, Optional\nimport uuid\nimport json\nimport asyncio\n\nfrom . import storage\nfrom .council import (\n    run_full_council,\n    generate_conversation_title,\n    stage1_collect_responses,\n    stage2_collect_rankings,\n    stage3_synthesize_final,\n    stage3_synthesize_streaming,\n    calculate_aggregate_rankings,\n    calculate_dissent_metrics\n)\nfrom .openrouter import CircuitBreaker, fetch_available_models, format_model_for_display\nfrom .db.session import init_db, migrate_from_json\n\n# Initialize database on module load\ninit_db()\n\n# Migrate existing JSON conversations if any\nmigrate_from_json()\n\napp = FastAPI(title=\"LLM Council API\", version=\"1.1.0\")\n\n# CORS origins - allow localhost for dev and configured frontend URL for production\ncors_origins = [\n    \"http://localhost:5173\",\n    \"http://localhost:3000\",\n    \"https://llmcouncil.digital\",\n    \"https://www.llmcouncil.digital\",\n]\n\n# Add production frontend URL if configured\nfrontend_url = os.environ.get(\"FRONTEND_URL\")\nif frontend_url:\n    cors_origins.append(frontend_url)\n    # Also allow without trailing slash\n    cors_origins.append(frontend_url.rstrip(\"/\"))\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=cors_origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n# Request/Response Models\n\nclass CreateConversationRequest(BaseModel):\n    \"\"\"Request to create a new conversation.\"\"\"\n    pass\n\n\nclass SendMessageRequest(BaseModel):\n    \"\"\"Request to send a message in a conversation.\"\"\"\n    content: str\n\n\nclass ConversationMetadata(BaseModel):\n    \"\"\"Conversation metadata for list view.\"\"\"\n    id: str\n    created_at: str\n    title: str\n    message_count: int\n\n\nclass Conversation(BaseModel):\n    \"\"\"Full conversation with all messages.\"\"\"\n    id: str\n    created_at: str\n    title: str\n    messages: List[Dict[str, Any]]\n\n\nclass RankingCriterion(BaseModel):\n    \"\"\"A single ranking criterion for Stage 2 evaluation.\"\"\"\n    id: str\n    name: str\n    description: str = \"\"\n    weight: float = 1.0\n    enabled: bool = True\n\n\nclass CouncilConfigRequest(BaseModel):\n    \"\"\"Request to update council configuration.\"\"\"\n    council_models: Optional[List[str]] = None\n    chairman_model: Optional[str] = None\n    theme: Optional[str] = None\n    # Phase 4: Advanced Deliberation\n    ranking_criteria: Optional[List[Dict[str, Any]]] = None\n    model_weights: Optional[Dict[str, float]] = None\n    enable_confidence: Optional[bool] = None\n    enable_dissent_tracking: Optional[bool] = None\n    # Phase 5: Model Management\n    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n\n\nclass CouncilConfigResponse(BaseModel):\n    \"\"\"Council configuration response.\"\"\"\n    council_models: List[str]\n    chairman_model: str\n    theme: str\n    # Phase 4: Advanced Deliberation\n    ranking_criteria: List[Dict[str, Any]] = []\n    model_weights: Dict[str, float] = {}\n    enable_confidence: bool = False\n    enable_dissent_tracking: bool = True\n    # Phase 5: Model Management\n    model_parameters: Dict[str, Dict[str, Any]] = {}\n\n\nclass ExportRequest(BaseModel):\n    \"\"\"Request to export a conversation.\"\"\"\n    format: str = \"markdown\"  # markdown, json\n\n\nclass PresetRequest(BaseModel):\n    \"\"\"Request to create or update a preset.\"\"\"\n    name: str\n    description: Optional[str] = None\n    council_models: List[str]\n    chairman_model: str\n    model_weights: Optional[Dict[str, float]] = None\n    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n    ranking_criteria: Optional[List[Dict[str, Any]]] = None\n\n\nclass PresetUpdateRequest(BaseModel):\n    \"\"\"Request to update a preset (all fields optional).\"\"\"\n    name: Optional[str] = None\n    description: Optional[str] = None\n    council_models: Optional[List[str]] = None\n    chairman_model: Optional[str] = None\n    model_weights: Optional[Dict[str, float]] = None\n    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n    ranking_criteria: Optional[List[Dict[str, Any]]] = None\n\n\n# Health Check\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Health check endpoint.\"\"\"\n    return {\"status\": \"ok\", \"service\": \"LLM Council API\", \"version\": \"1.1.0\"}\n\n\n# Conversation Endpoints\n\n@app.get(\"/api/conversations\", response_model=List[ConversationMetadata])\nasync def list_conversations():\n    \"\"\"List all conversations (metadata only).\"\"\"\n    return storage.list_conversations()\n\n\n@app.post(\"/api/conversations\", response_model=Conversation)\nasync def create_conversation(request: CreateConversationRequest):\n    \"\"\"Create a new conversation.\"\"\"\n    conversation_id = str(uuid.uuid4())\n    conversation = storage.create_conversation(conversation_id)\n    return conversation\n\n\n@app.get(\"/api/conversations/{conversation_id}\", response_model=Conversation)\nasync def get_conversation(conversation_id: str):\n    \"\"\"Get a specific conversation with all its messages.\"\"\"\n    conversation = storage.get_conversation(conversation_id)\n    if conversation is None:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n    return conversation\n\n\n@app.delete(\"/api/conversations/{conversation_id}\")\nasync def delete_conversation(conversation_id: str):\n    \"\"\"Delete a conversation.\"\"\"\n    deleted = storage.delete_conversation(conversation_id)\n    if not deleted:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n    return {\"status\": \"deleted\", \"id\": conversation_id}\n\n\n@app.post(\"/api/conversations/{conversation_id}/message\")\nasync def send_message(conversation_id: str, request: SendMessageRequest):\n    \"\"\"\n    Send a message and run the 3-stage council process.\n    Returns the complete response with all stages.\n    \"\"\"\n    # Check if conversation exists\n    conversation = storage.get_conversation(conversation_id)\n    if conversation is None:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n    # Check if this is the first message\n    is_first_message = len(conversation[\"messages\"]) == 0\n\n    # Add user message\n    storage.add_user_message(conversation_id, request.content)\n\n    # If this is the first message, generate a title\n    if is_first_message:\n        title = await generate_conversation_title(request.content)\n        storage.update_conversation_title(conversation_id, title)\n\n    # Load council config for Phase 4 features\n    config = storage.get_council_config()\n\n    # Run the 3-stage council process with Phase 4 settings\n    stage1_results, stage2_results, stage3_result, metadata, all_metrics = await run_full_council(\n        request.content,\n        ranking_criteria=config.get(\"ranking_criteria\"),\n        model_weights=config.get(\"model_weights\"),\n        enable_dissent_tracking=config.get(\"enable_dissent_tracking\", True),\n        enable_confidence=config.get(\"enable_confidence\", False)\n    )\n\n    # Add assistant message with all stages\n    storage.add_assistant_message(\n        conversation_id,\n        stage1_results,\n        stage2_results,\n        stage3_result,\n        metadata\n    )\n\n    # Save metrics\n    message_id = storage.get_last_message_id(conversation_id)\n    if message_id and all_metrics:\n        storage.save_model_metrics(message_id, all_metrics)\n\n    # Return the complete response with metadata\n    return {\n        \"stage1\": stage1_results,\n        \"stage2\": stage2_results,\n        \"stage3\": stage3_result,\n        \"metadata\": metadata\n    }\n\n\ndef get_conversation_history(conversation: Dict[str, Any]) -> List[Dict[str, str]]:\n    \"\"\"\n    Extract conversation history for multi-turn context.\n\n    Returns list of {role, content} dicts suitable for LLM API.\n    \"\"\"\n    history = []\n    for msg in conversation.get(\"messages\", []):\n        if msg[\"role\"] == \"user\":\n            history.append({\n                \"role\": \"user\",\n                \"content\": msg.get(\"content\", \"\")\n            })\n        elif msg[\"role\"] == \"assistant\":\n            # Use the stage3 synthesis as the assistant response\n            if msg.get(\"stage3\"):\n                history.append({\n                    \"role\": \"assistant\",\n                    \"content\": msg[\"stage3\"].get(\"response\", \"\")\n                })\n    return history\n\n\n@app.post(\"/api/conversations/{conversation_id}/message/stream\")\nasync def send_message_stream(conversation_id: str, request: SendMessageRequest):\n    \"\"\"\n    Send a message and stream the 3-stage council process.\n    Returns Server-Sent Events as each stage completes.\n    Supports multi-turn conversation context.\n    \"\"\"\n    # Check if conversation exists\n    conversation = storage.get_conversation(conversation_id)\n    if conversation is None:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n    # Check if this is the first message\n    is_first_message = len(conversation[\"messages\"]) == 0\n\n    # Get conversation history for multi-turn context\n    conversation_history = get_conversation_history(conversation)\n\n    # Load council config for Phase 4 features\n    config = storage.get_council_config()\n    ranking_criteria = config.get(\"ranking_criteria\")\n    model_weights = config.get(\"model_weights\")\n    enable_dissent_tracking = config.get(\"enable_dissent_tracking\", True)\n    enable_confidence = config.get(\"enable_confidence\", False)\n\n    async def event_generator():\n        try:\n            all_metrics = []\n\n            # Add user message\n            storage.add_user_message(conversation_id, request.content)\n\n            # Start title generation in parallel (don't await yet)\n            title_task = None\n            if is_first_message:\n                title_task = asyncio.create_task(generate_conversation_title(request.content))\n\n            # Stage 1: Collect responses (with conversation history)\n            yield f\"data: {json.dumps({'type': 'stage1_start'})}\\n\\n\"\n            stage1_results, stage1_metrics = await stage1_collect_responses(\n                request.content,\n                conversation_history=conversation_history,\n                enable_confidence=enable_confidence\n            )\n            all_metrics.extend(stage1_metrics)\n            yield f\"data: {json.dumps({'type': 'stage1_complete', 'data': stage1_results})}\\n\\n\"\n\n            # Stage 2: Collect rankings with Phase 4 custom criteria\n            yield f\"data: {json.dumps({'type': 'stage2_start'})}\\n\\n\"\n            stage2_results, label_to_model, stage2_metrics = await stage2_collect_rankings(\n                request.content,\n                stage1_results,\n                ranking_criteria=ranking_criteria\n            )\n            all_metrics.extend(stage2_metrics)\n\n            # Calculate aggregate rankings with Phase 4 weighted voting\n            aggregate_rankings = calculate_aggregate_rankings(\n                stage2_results,\n                label_to_model,\n                model_weights=model_weights\n            )\n\n            # Add ranking positions to stage1 metrics\n            for ranking in aggregate_rankings:\n                for metric in all_metrics:\n                    if metric['model_id'] == ranking['model'] and metric.get('stage') == 'stage1':\n                        metric['ranking_position'] = ranking['average_rank']\n\n            # Build metadata with optional dissent tracking\n            stage2_metadata = {\n                'label_to_model': label_to_model,\n                'aggregate_rankings': aggregate_rankings\n            }\n\n            if enable_dissent_tracking and len(stage2_results) >= 2:\n                dissent_metrics = calculate_dissent_metrics(stage2_results, label_to_model)\n                stage2_metadata['dissent'] = dissent_metrics\n\n            yield f\"data: {json.dumps({'type': 'stage2_complete', 'data': stage2_results, 'metadata': stage2_metadata})}\\n\\n\"\n\n            # Stage 3: Synthesize final answer\n            yield f\"data: {json.dumps({'type': 'stage3_start'})}\\n\\n\"\n            stage3_result, stage3_metrics = await stage3_synthesize_final(request.content, stage1_results, stage2_results)\n            all_metrics.append(stage3_metrics)\n            yield f\"data: {json.dumps({'type': 'stage3_complete', 'data': stage3_result})}\\n\\n\"\n\n            # Wait for title generation if it was started\n            if title_task:\n                title = await title_task\n                storage.update_conversation_title(conversation_id, title)\n                yield f\"data: {json.dumps({'type': 'title_complete', 'data': {'title': title}})}\\n\\n\"\n\n            # Save complete assistant message with metadata\n            metadata = {\n                \"label_to_model\": label_to_model,\n                \"aggregate_rankings\": aggregate_rankings\n            }\n            if enable_dissent_tracking and len(stage2_results) >= 2:\n                metadata[\"dissent\"] = stage2_metadata.get(\"dissent\")\n\n            storage.add_assistant_message(\n                conversation_id,\n                stage1_results,\n                stage2_results,\n                stage3_result,\n                metadata\n            )\n\n            # Save metrics\n            message_id = storage.get_last_message_id(conversation_id)\n            if message_id and all_metrics:\n                storage.save_model_metrics(message_id, all_metrics)\n\n            # Send completion event\n            yield f\"data: {json.dumps({'type': 'complete'})}\\n\\n\"\n\n        except Exception as e:\n            # Send error event\n            yield f\"data: {json.dumps({'type': 'error', 'message': str(e)})}\\n\\n\"\n\n    return StreamingResponse(\n        event_generator(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n        }\n    )\n\n\n@app.post(\"/api/conversations/{conversation_id}/message/stream-tokens\")\nasync def send_message_stream_tokens(conversation_id: str, request: SendMessageRequest):\n    \"\"\"\n    Send a message with full token-by-token streaming for Stage 3.\n    Returns Server-Sent Events including individual tokens during synthesis.\n    \"\"\"\n    # Check if conversation exists\n    conversation = storage.get_conversation(conversation_id)\n    if conversation is None:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n    is_first_message = len(conversation[\"messages\"]) == 0\n    conversation_history = get_conversation_history(conversation)\n\n    # Load council config for Phase 4 features\n    config = storage.get_council_config()\n    ranking_criteria = config.get(\"ranking_criteria\")\n    model_weights = config.get(\"model_weights\")\n    enable_dissent_tracking = config.get(\"enable_dissent_tracking\", True)\n    enable_confidence = config.get(\"enable_confidence\", False)\n\n    async def event_generator():\n        try:\n            all_metrics = []\n            storage.add_user_message(conversation_id, request.content)\n\n            title_task = None\n            if is_first_message:\n                title_task = asyncio.create_task(generate_conversation_title(request.content))\n\n            # Stage 1\n            yield f\"data: {json.dumps({'type': 'stage1_start'})}\\n\\n\"\n            stage1_results, stage1_metrics = await stage1_collect_responses(\n                request.content,\n                conversation_history=conversation_history,\n                enable_confidence=enable_confidence\n            )\n            all_metrics.extend(stage1_metrics)\n            yield f\"data: {json.dumps({'type': 'stage1_complete', 'data': stage1_results})}\\n\\n\"\n\n            # Stage 2 with Phase 4 custom criteria\n            yield f\"data: {json.dumps({'type': 'stage2_start'})}\\n\\n\"\n            stage2_results, label_to_model, stage2_metrics = await stage2_collect_rankings(\n                request.content,\n                stage1_results,\n                ranking_criteria=ranking_criteria\n            )\n            all_metrics.extend(stage2_metrics)\n\n            # Calculate aggregate rankings with Phase 4 weighted voting\n            aggregate_rankings = calculate_aggregate_rankings(\n                stage2_results,\n                label_to_model,\n                model_weights=model_weights\n            )\n\n            # Add ranking positions to stage1 metrics\n            for ranking in aggregate_rankings:\n                for metric in all_metrics:\n                    if metric['model_id'] == ranking['model'] and metric.get('stage') == 'stage1':\n                        metric['ranking_position'] = ranking['average_rank']\n\n            # Build metadata with optional dissent tracking\n            stage2_metadata = {\n                'label_to_model': label_to_model,\n                'aggregate_rankings': aggregate_rankings\n            }\n\n            if enable_dissent_tracking and len(stage2_results) >= 2:\n                dissent_metrics = calculate_dissent_metrics(stage2_results, label_to_model)\n                stage2_metadata['dissent'] = dissent_metrics\n\n            yield f\"data: {json.dumps({'type': 'stage2_complete', 'data': stage2_results, 'metadata': stage2_metadata})}\\n\\n\"\n\n            # Stage 3 with token streaming\n            yield f\"data: {json.dumps({'type': 'stage3_start'})}\\n\\n\"\n\n            full_response = \"\"\n            chairman_model = \"\"\n\n            async for chunk in stage3_synthesize_streaming(\n                request.content, stage1_results, stage2_results\n            ):\n                chairman_model = chunk.get(\"model\", \"\")\n\n                if chunk[\"type\"] == \"token\":\n                    full_response += chunk[\"content\"]\n                    yield f\"data: {json.dumps({'type': 'stage3_token', 'data': {'content': chunk['content']}})}\\n\\n\"\n                elif chunk[\"type\"] == \"done\":\n                    full_response = chunk[\"content\"]\n                elif chunk[\"type\"] == \"error\":\n                    yield f\"data: {json.dumps({'type': 'error', 'message': chunk['content']})}\\n\\n\"\n                    return\n\n            stage3_result = {\n                \"model\": chairman_model,\n                \"response\": full_response\n            }\n            yield f\"data: {json.dumps({'type': 'stage3_complete', 'data': stage3_result})}\\n\\n\"\n\n            # Title generation\n            if title_task:\n                title = await title_task\n                storage.update_conversation_title(conversation_id, title)\n                yield f\"data: {json.dumps({'type': 'title_complete', 'data': {'title': title}})}\\n\\n\"\n\n            # Save message with Phase 4 metadata\n            metadata = {\n                \"label_to_model\": label_to_model,\n                \"aggregate_rankings\": aggregate_rankings\n            }\n            if enable_dissent_tracking and len(stage2_results) >= 2:\n                metadata[\"dissent\"] = stage2_metadata.get(\"dissent\")\n\n            storage.add_assistant_message(\n                conversation_id,\n                stage1_results,\n                stage2_results,\n                stage3_result,\n                metadata\n            )\n\n            # Save metrics (stage3 metrics from streaming are minimal)\n            stage3_metrics = {\n                'model_id': chairman_model,\n                'stage': 'stage3',\n            }\n            all_metrics.append(stage3_metrics)\n\n            message_id = storage.get_last_message_id(conversation_id)\n            if message_id and all_metrics:\n                storage.save_model_metrics(message_id, all_metrics)\n\n            yield f\"data: {json.dumps({'type': 'complete'})}\\n\\n\"\n\n        except Exception as e:\n            yield f\"data: {json.dumps({'type': 'error', 'message': str(e)})}\\n\\n\"\n\n    return StreamingResponse(\n        event_generator(),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n        }\n    )\n\n\n@app.get(\"/api/circuit-breaker/status\")\nasync def get_circuit_breaker_status():\n    \"\"\"Get circuit breaker status for all models.\"\"\"\n    from .config import get_council_models, get_chairman_model\n\n    models = get_council_models() + [get_chairman_model()]\n    status = {}\n\n    for model in set(models):\n        status[model] = CircuitBreaker.get_status(model)\n\n    return {\"circuit_breaker_status\": status}\n\n\n# Model Discovery Endpoints\n\n@app.get(\"/api/models\")\nasync def get_available_models(\n    search: Optional[str] = None,\n    refresh: bool = False\n):\n    \"\"\"\n    Get available models from OpenRouter.\n\n    Args:\n        search: Optional search term to filter models by name or id\n        refresh: Force refresh from API instead of using cache\n\n    Returns:\n        List of available models with metadata\n    \"\"\"\n    raw_models = await fetch_available_models(force_refresh=refresh)\n\n    # Format models for display\n    models = [format_model_for_display(m) for m in raw_models]\n\n    # Filter by search term if provided\n    if search:\n        search_lower = search.lower()\n        models = [\n            m for m in models\n            if search_lower in m[\"id\"].lower() or search_lower in m[\"name\"].lower()\n        ]\n\n    # Sort by name\n    models.sort(key=lambda m: m[\"name\"].lower())\n\n    return {\"models\": models, \"count\": len(models)}\n\n\n@app.get(\"/api/models/popular\")\nasync def get_popular_models():\n    \"\"\"Get a curated list of popular models for quick selection.\"\"\"\n    popular_ids = [\n        \"openai/gpt-4o\",\n        \"openai/gpt-4o-mini\",\n        \"openai/o1\",\n        \"openai/o1-mini\",\n        \"anthropic/claude-3.5-sonnet\",\n        \"anthropic/claude-3.5-haiku\",\n        \"anthropic/claude-3-opus\",\n        \"google/gemini-2.5-flash\",\n        \"google/gemini-2.5-pro\",\n        \"meta-llama/llama-3.3-70b-instruct\",\n        \"deepseek/deepseek-chat\",\n        \"deepseek/deepseek-r1\",\n        \"x-ai/grok-3\",\n        \"x-ai/grok-3-mini\",\n        \"mistralai/mistral-large\",\n        \"qwen/qwen-2.5-72b-instruct\",\n    ]\n\n    raw_models = await fetch_available_models()\n\n    # Find matching models\n    popular = []\n    model_map = {m.get(\"id\"): m for m in raw_models}\n\n    for model_id in popular_ids:\n        if model_id in model_map:\n            popular.append(format_model_for_display(model_map[model_id]))\n\n    return {\"models\": popular}\n\n\n# Settings Endpoints\n\n@app.get(\"/api/settings/council\", response_model=CouncilConfigResponse)\nasync def get_council_config():\n    \"\"\"Get the current council configuration.\"\"\"\n    config = storage.get_council_config()\n    return CouncilConfigResponse(\n        council_models=config.get(\"council_models\", []),\n        chairman_model=config.get(\"chairman_model\", \"\"),\n        theme=config.get(\"theme\", \"light\"),\n        ranking_criteria=config.get(\"ranking_criteria\", []),\n        model_weights=config.get(\"model_weights\", {}),\n        model_parameters=config.get(\"model_parameters\", {}),\n        enable_confidence=config.get(\"enable_confidence\", False),\n        enable_dissent_tracking=config.get(\"enable_dissent_tracking\", True)\n    )\n\n\n@app.put(\"/api/settings/council\", response_model=CouncilConfigResponse)\nasync def update_council_config(request: CouncilConfigRequest):\n    \"\"\"Update the council configuration.\"\"\"\n    config = storage.update_council_config(\n        council_models=request.council_models,\n        chairman_model=request.chairman_model,\n        theme=request.theme,\n        ranking_criteria=request.ranking_criteria,\n        model_weights=request.model_weights,\n        model_parameters=request.model_parameters,\n        enable_confidence=request.enable_confidence,\n        enable_dissent_tracking=request.enable_dissent_tracking\n    )\n    return CouncilConfigResponse(\n        council_models=config.get(\"council_models\", []),\n        chairman_model=config.get(\"chairman_model\", \"\"),\n        theme=config.get(\"theme\", \"light\"),\n        ranking_criteria=config.get(\"ranking_criteria\", []),\n        model_weights=config.get(\"model_weights\", {}),\n        model_parameters=config.get(\"model_parameters\", {}),\n        enable_confidence=config.get(\"enable_confidence\", False),\n        enable_dissent_tracking=config.get(\"enable_dissent_tracking\", True)\n    )\n\n\n# Preset Endpoints\n\n@app.get(\"/api/presets\")\nasync def list_presets():\n    \"\"\"List all presets.\"\"\"\n    return storage.list_presets()\n\n\n@app.get(\"/api/presets/default\")\nasync def get_default_preset():\n    \"\"\"Get the currently active preset.\"\"\"\n    preset = storage.get_default_preset()\n    if preset is None:\n        return {\"preset\": None}\n    return {\"preset\": preset}\n\n\n@app.get(\"/api/presets/{preset_id}\")\nasync def get_preset(preset_id: int):\n    \"\"\"Get a specific preset.\"\"\"\n    preset = storage.get_preset(preset_id)\n    if preset is None:\n        raise HTTPException(status_code=404, detail=\"Preset not found\")\n    return preset\n\n\n@app.post(\"/api/presets\")\nasync def create_preset(request: PresetRequest):\n    \"\"\"Create a new preset.\"\"\"\n    preset = storage.create_preset(\n        name=request.name,\n        description=request.description,\n        council_models=request.council_models,\n        chairman_model=request.chairman_model,\n        model_weights=request.model_weights,\n        model_parameters=request.model_parameters,\n        ranking_criteria=request.ranking_criteria\n    )\n    return preset\n\n\n@app.put(\"/api/presets/{preset_id}\")\nasync def update_preset(preset_id: int, request: PresetUpdateRequest):\n    \"\"\"Update an existing preset.\"\"\"\n    preset = storage.update_preset(\n        preset_id=preset_id,\n        name=request.name,\n        description=request.description,\n        council_models=request.council_models,\n        chairman_model=request.chairman_model,\n        model_weights=request.model_weights,\n        model_parameters=request.model_parameters,\n        ranking_criteria=request.ranking_criteria\n    )\n    if preset is None:\n        raise HTTPException(status_code=404, detail=\"Preset not found\")\n    return preset\n\n\n@app.delete(\"/api/presets/{preset_id}\")\nasync def delete_preset(preset_id: int):\n    \"\"\"Delete a preset.\"\"\"\n    deleted = storage.delete_preset(preset_id)\n    if not deleted:\n        raise HTTPException(status_code=404, detail=\"Preset not found\")\n    return {\"status\": \"deleted\", \"id\": preset_id}\n\n\n@app.post(\"/api/presets/{preset_id}/apply\")\nasync def apply_preset(preset_id: int):\n    \"\"\"Apply a preset to the current council configuration.\"\"\"\n    config = storage.apply_preset(preset_id)\n    if config is None:\n        raise HTTPException(status_code=404, detail=\"Preset not found\")\n    return {\"status\": \"applied\", \"config\": config}\n\n\n@app.post(\"/api/presets/save-current\")\nasync def save_current_as_preset(name: str, description: Optional[str] = None):\n    \"\"\"Save the current council configuration as a new preset.\"\"\"\n    config = storage.get_council_config()\n    preset = storage.create_preset(\n        name=name,\n        description=description,\n        council_models=config.get(\"council_models\", []),\n        chairman_model=config.get(\"chairman_model\", \"\"),\n        model_weights=config.get(\"model_weights\"),\n        model_parameters=config.get(\"model_parameters\"),\n        ranking_criteria=config.get(\"ranking_criteria\")\n    )\n    return preset\n\n\n# Export Endpoints\n\n@app.post(\"/api/conversations/{conversation_id}/export\")\nasync def export_conversation(conversation_id: str, request: ExportRequest):\n    \"\"\"Export a conversation to the specified format.\"\"\"\n    conversation = storage.get_conversation(conversation_id)\n    if conversation is None:\n        raise HTTPException(status_code=404, detail=\"Conversation not found\")\n\n    if request.format == \"json\":\n        return conversation\n\n    elif request.format == \"markdown\":\n        md = generate_markdown_export(conversation)\n        return StreamingResponse(\n            iter([md]),\n            media_type=\"text/markdown\",\n            headers={\n                \"Content-Disposition\": f\"attachment; filename={conversation_id}.md\"\n            }\n        )\n\n    else:\n        raise HTTPException(status_code=400, detail=f\"Unsupported format: {request.format}\")\n\n\ndef generate_markdown_export(conversation: Dict[str, Any]) -> str:\n    \"\"\"Generate markdown export of a conversation.\"\"\"\n    lines = [\n        f\"# {conversation['title']}\",\n        f\"\",\n        f\"*Exported from LLM Council*\",\n        f\"*Created: {conversation['created_at']}*\",\n        f\"\",\n        \"---\",\n        \"\"\n    ]\n\n    for msg in conversation[\"messages\"]:\n        if msg[\"role\"] == \"user\":\n            lines.extend([\n                \"## User\",\n                \"\",\n                msg[\"content\"],\n                \"\",\n                \"---\",\n                \"\"\n            ])\n        else:\n            lines.extend([\n                \"## LLM Council Response\",\n                \"\",\n                \"### Stage 1: Individual Responses\",\n                \"\"\n            ])\n\n            if msg.get(\"stage1\"):\n                for resp in msg[\"stage1\"]:\n                    lines.extend([\n                        f\"#### {resp['model']}\",\n                        \"\",\n                        resp[\"response\"],\n                        \"\"\n                    ])\n\n            lines.extend([\n                \"### Stage 2: Peer Rankings\",\n                \"\"\n            ])\n\n            if msg.get(\"stage2\"):\n                for ranking in msg[\"stage2\"]:\n                    lines.extend([\n                        f\"#### {ranking['model']}\",\n                        \"\",\n                        ranking[\"ranking\"],\n                        \"\"\n                    ])\n\n            lines.extend([\n                \"### Stage 3: Final Synthesis\",\n                \"\"\n            ])\n\n            if msg.get(\"stage3\"):\n                lines.extend([\n                    f\"*Chairman: {msg['stage3']['model']}*\",\n                    \"\",\n                    msg[\"stage3\"][\"response\"],\n                    \"\"\n                ])\n\n            lines.extend([\n                \"---\",\n                \"\"\n            ])\n\n    return \"\\n\".join(lines)\n\n\n# Analytics Endpoints\n\n@app.get(\"/api/analytics/summary\")\nasync def get_analytics_summary():\n    \"\"\"Get aggregate analytics summary across all conversations.\"\"\"\n    return storage.get_analytics_summary()\n\n\n@app.get(\"/api/analytics/models/{model_id:path}\")\nasync def get_model_analytics(model_id: str):\n    \"\"\"Get detailed analytics for a specific model.\"\"\"\n    return storage.get_model_analytics(model_id)\n\n\n@app.get(\"/api/analytics/recent\")\nasync def get_recent_metrics(limit: int = 100):\n    \"\"\"Get recent metrics for timeline display.\"\"\"\n    return storage.get_recent_metrics(limit)\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n    port = int(os.environ.get(\"PORT\", 8001))\n    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n","numLines":897,"startLine":1,"totalLines":897}}}
{"parentUuid":"30f2ec4e-db00-40af-a7c3-4ca357460fa5","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01PsKoZFEpEdKFKLDBfFHNAZ","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me examine the openrouter.py file to see how models are queried:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":11259,"cache_read_input_tokens":81630,"cache_creation":{"ephemeral_5m_input_tokens":11259,"ephemeral_1h_input_tokens":0},"output_tokens":6,"service_tier":"standard"}},"requestId":"req_011CWhmPmMGiShUF35iF892J","type":"assistant","uuid":"1f2e9c63-d34e-4d07-9095-4bc77309c3e6","timestamp":"2026-01-02T02:31:14.998Z"}
{"parentUuid":"1f2e9c63-d34e-4d07-9095-4bc77309c3e6","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01PsKoZFEpEdKFKLDBfFHNAZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01HskPV75HrTti94RzFm7R4N","name":"Read","input":{"file_path":"/Users/jeremy/Projects/LLMCouncil/backend/openrouter.py","limit":200}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":11259,"cache_read_input_tokens":81630,"cache_creation":{"ephemeral_5m_input_tokens":11259,"ephemeral_1h_input_tokens":0},"output_tokens":6,"service_tier":"standard"}},"requestId":"req_011CWhmPmMGiShUF35iF892J","type":"assistant","uuid":"4cfd2ef2-0f6e-4678-b4b0-8cf19daa37ac","timestamp":"2026-01-02T02:31:15.417Z"}
{"parentUuid":"4cfd2ef2-0f6e-4678-b4b0-8cf19daa37ac","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01HskPV75HrTti94RzFm7R4N","type":"tool_result","content":"     1→\"\"\"OpenRouter API client for making LLM requests.\"\"\"\n     2→\n     3→import httpx\n     4→import json\n     5→import time\n     6→import re\n     7→from typing import List, Dict, Any, Optional, AsyncGenerator\n     8→from .config import OPENROUTER_API_KEY, OPENROUTER_API_URL\n     9→\n    10→# OpenRouter models endpoint\n    11→OPENROUTER_MODELS_URL = \"https://openrouter.ai/api/v1/models\"\n    12→\n    13→# Cache for models list\n    14→_models_cache: Optional[Dict[str, Any]] = None\n    15→_models_cache_time: float = 0\n    16→MODELS_CACHE_TTL = 300  # 5 minutes\n    17→\n    18→\n    19→# Patterns to identify reasoning models\n    20→REASONING_MODEL_PATTERNS = [\n    21→    r'openai/o1',\n    22→    r'openai/o3',\n    23→    r'o1-preview',\n    24→    r'o1-mini',\n    25→    r'o3-mini',\n    26→    r'reasoning',\n    27→    r'thinking',\n    28→    r'deepseek.*reasoner',\n    29→]\n    30→\n    31→\n    32→def is_reasoning_model(model: str) -> bool:\n    33→    \"\"\"\n    34→    Check if a model is a reasoning model that requires special handling.\n    35→\n    36→    Reasoning models (o1, o3, etc.) have different API requirements:\n    37→    - No system messages (must be converted to user messages)\n    38→    - Different temperature handling\n    39→    - May return reasoning_content in addition to content\n    40→    \"\"\"\n    41→    model_lower = model.lower()\n    42→    return any(re.search(pattern, model_lower) for pattern in REASONING_MODEL_PATTERNS)\n    43→\n    44→\n    45→async def fetch_available_models(\n    46→    force_refresh: bool = False\n    47→) -> List[Dict[str, Any]]:\n    48→    \"\"\"\n    49→    Fetch available models from OpenRouter API.\n    50→\n    51→    Returns a list of model objects with id, name, description, pricing, etc.\n    52→    Results are cached for 5 minutes unless force_refresh is True.\n    53→    \"\"\"\n    54→    global _models_cache, _models_cache_time\n    55→\n    56→    # Check cache\n    57→    if not force_refresh and _models_cache is not None:\n    58→        if time.time() - _models_cache_time < MODELS_CACHE_TTL:\n    59→            return _models_cache.get(\"data\", [])\n    60→\n    61→    headers = {\n    62→        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n    63→        \"Content-Type\": \"application/json\",\n    64→    }\n    65→\n    66→    try:\n    67→        async with httpx.AsyncClient(timeout=30.0) as client:\n    68→            response = await client.get(\n    69→                OPENROUTER_MODELS_URL,\n    70→                headers=headers\n    71→            )\n    72→            response.raise_for_status()\n    73→\n    74→            data = response.json()\n    75→            _models_cache = data\n    76→            _models_cache_time = time.time()\n    77→\n    78→            return data.get(\"data\", [])\n    79→\n    80→    except Exception as e:\n    81→        print(f\"Error fetching models from OpenRouter: {e}\")\n    82→        # Return cached data if available, otherwise empty list\n    83→        if _models_cache is not None:\n    84→            return _models_cache.get(\"data\", [])\n    85→        return []\n    86→\n    87→\n    88→def format_model_for_display(model: Dict[str, Any]) -> Dict[str, Any]:\n    89→    \"\"\"\n    90→    Format a model from OpenRouter API for frontend display.\n    91→\n    92→    Extracts key information and adds computed fields like is_reasoning.\n    93→    \"\"\"\n    94→    model_id = model.get(\"id\", \"\")\n    95→    pricing = model.get(\"pricing\", {})\n    96→\n    97→    # Calculate cost per 1M tokens\n    98→    prompt_cost = float(pricing.get(\"prompt\", 0)) * 1_000_000\n    99→    completion_cost = float(pricing.get(\"completion\", 0)) * 1_000_000\n   100→\n   101→    return {\n   102→        \"id\": model_id,\n   103→        \"name\": model.get(\"name\", model_id),\n   104→        \"description\": model.get(\"description\", \"\"),\n   105→        \"context_length\": model.get(\"context_length\", 0),\n   106→        \"is_reasoning\": is_reasoning_model(model_id),\n   107→        \"pricing\": {\n   108→            \"prompt_per_million\": prompt_cost,\n   109→            \"completion_per_million\": completion_cost,\n   110→        },\n   111→        \"top_provider\": model.get(\"top_provider\", {}),\n   112→    }\n   113→\n   114→\n   115→def prepare_messages_for_model(\n   116→    model: str,\n   117→    messages: List[Dict[str, str]]\n   118→) -> List[Dict[str, str]]:\n   119→    \"\"\"\n   120→    Prepare messages for a specific model, handling reasoning model requirements.\n   121→\n   122→    For reasoning models:\n   123→    - Convert system messages to user messages (prepended with context)\n   124→    - Merge consecutive user messages\n   125→    \"\"\"\n   126→    if not is_reasoning_model(model):\n   127→        return messages\n   128→\n   129→    # Convert system messages to user messages for reasoning models\n   130→    converted = []\n   131→    for msg in messages:\n   132→        if msg['role'] == 'system':\n   133→            # Convert system message to user message with clear labeling\n   134→            converted.append({\n   135→                'role': 'user',\n   136→                'content': f\"[System Context]\\n{msg['content']}\"\n   137→            })\n   138→        else:\n   139→            converted.append(msg.copy())\n   140→\n   141→    # Merge consecutive user messages (reasoning models may require alternating roles)\n   142→    merged = []\n   143→    for msg in converted:\n   144→        if merged and merged[-1]['role'] == msg['role'] == 'user':\n   145→            merged[-1]['content'] += '\\n\\n' + msg['content']\n   146→        else:\n   147→            merged.append(msg)\n   148→\n   149→    return merged\n   150→\n   151→\n   152→def get_reasoning_model_params(model: str) -> Dict[str, Any]:\n   153→    \"\"\"\n   154→    Get reasoning model-specific parameters for the API request.\n   155→\n   156→    Returns parameters dict to merge with the payload for reasoning models.\n   157→    \"\"\"\n   158→    params = {}\n   159→\n   160→    if is_reasoning_model(model):\n   161→        # Reasoning models don't support temperature\n   162→        # Some may support reasoning_effort\n   163→        params['reasoning_effort'] = 'high'\n   164→\n   165→    return params\n   166→\n   167→\n   168→async def query_model(\n   169→    model: str,\n   170→    messages: List[Dict[str, str]],\n   171→    timeout: float = 120.0,\n   172→    temperature: Optional[float] = None,\n   173→    max_tokens: Optional[int] = None\n   174→) -> Optional[Dict[str, Any]]:\n   175→    \"\"\"\n   176→    Query a single model via OpenRouter API.\n   177→\n   178→    Args:\n   179→        model: OpenRouter model identifier (e.g., \"openai/gpt-4o\")\n   180→        messages: List of message dicts with 'role' and 'content'\n   181→        timeout: Request timeout in seconds\n   182→        temperature: Optional temperature for response generation (0.0-2.0)\n   183→        max_tokens: Optional maximum tokens in response\n   184→\n   185→    Returns:\n   186→        Response dict with 'content', optional 'reasoning_details', and metrics, or None if failed\n   187→    \"\"\"\n   188→    headers = {\n   189→        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n   190→        \"Content-Type\": \"application/json\",\n   191→    }\n   192→\n   193→    # Prepare messages for this specific model (handles reasoning models)\n   194→    prepared_messages = prepare_messages_for_model(model, messages)\n   195→\n   196→    # Build payload with model-specific parameters\n   197→    payload = {\n   198→        \"model\": model,\n   199→        \"messages\": prepared_messages,\n   200→    }\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"7176d923-acb6-476f-9ff9-61a4ff925f2b","timestamp":"2026-01-02T02:31:15.679Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/jeremy/Projects/LLMCouncil/backend/openrouter.py","content":"\"\"\"OpenRouter API client for making LLM requests.\"\"\"\n\nimport httpx\nimport json\nimport time\nimport re\nfrom typing import List, Dict, Any, Optional, AsyncGenerator\nfrom .config import OPENROUTER_API_KEY, OPENROUTER_API_URL\n\n# OpenRouter models endpoint\nOPENROUTER_MODELS_URL = \"https://openrouter.ai/api/v1/models\"\n\n# Cache for models list\n_models_cache: Optional[Dict[str, Any]] = None\n_models_cache_time: float = 0\nMODELS_CACHE_TTL = 300  # 5 minutes\n\n\n# Patterns to identify reasoning models\nREASONING_MODEL_PATTERNS = [\n    r'openai/o1',\n    r'openai/o3',\n    r'o1-preview',\n    r'o1-mini',\n    r'o3-mini',\n    r'reasoning',\n    r'thinking',\n    r'deepseek.*reasoner',\n]\n\n\ndef is_reasoning_model(model: str) -> bool:\n    \"\"\"\n    Check if a model is a reasoning model that requires special handling.\n\n    Reasoning models (o1, o3, etc.) have different API requirements:\n    - No system messages (must be converted to user messages)\n    - Different temperature handling\n    - May return reasoning_content in addition to content\n    \"\"\"\n    model_lower = model.lower()\n    return any(re.search(pattern, model_lower) for pattern in REASONING_MODEL_PATTERNS)\n\n\nasync def fetch_available_models(\n    force_refresh: bool = False\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Fetch available models from OpenRouter API.\n\n    Returns a list of model objects with id, name, description, pricing, etc.\n    Results are cached for 5 minutes unless force_refresh is True.\n    \"\"\"\n    global _models_cache, _models_cache_time\n\n    # Check cache\n    if not force_refresh and _models_cache is not None:\n        if time.time() - _models_cache_time < MODELS_CACHE_TTL:\n            return _models_cache.get(\"data\", [])\n\n    headers = {\n        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n        \"Content-Type\": \"application/json\",\n    }\n\n    try:\n        async with httpx.AsyncClient(timeout=30.0) as client:\n            response = await client.get(\n                OPENROUTER_MODELS_URL,\n                headers=headers\n            )\n            response.raise_for_status()\n\n            data = response.json()\n            _models_cache = data\n            _models_cache_time = time.time()\n\n            return data.get(\"data\", [])\n\n    except Exception as e:\n        print(f\"Error fetching models from OpenRouter: {e}\")\n        # Return cached data if available, otherwise empty list\n        if _models_cache is not None:\n            return _models_cache.get(\"data\", [])\n        return []\n\n\ndef format_model_for_display(model: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Format a model from OpenRouter API for frontend display.\n\n    Extracts key information and adds computed fields like is_reasoning.\n    \"\"\"\n    model_id = model.get(\"id\", \"\")\n    pricing = model.get(\"pricing\", {})\n\n    # Calculate cost per 1M tokens\n    prompt_cost = float(pricing.get(\"prompt\", 0)) * 1_000_000\n    completion_cost = float(pricing.get(\"completion\", 0)) * 1_000_000\n\n    return {\n        \"id\": model_id,\n        \"name\": model.get(\"name\", model_id),\n        \"description\": model.get(\"description\", \"\"),\n        \"context_length\": model.get(\"context_length\", 0),\n        \"is_reasoning\": is_reasoning_model(model_id),\n        \"pricing\": {\n            \"prompt_per_million\": prompt_cost,\n            \"completion_per_million\": completion_cost,\n        },\n        \"top_provider\": model.get(\"top_provider\", {}),\n    }\n\n\ndef prepare_messages_for_model(\n    model: str,\n    messages: List[Dict[str, str]]\n) -> List[Dict[str, str]]:\n    \"\"\"\n    Prepare messages for a specific model, handling reasoning model requirements.\n\n    For reasoning models:\n    - Convert system messages to user messages (prepended with context)\n    - Merge consecutive user messages\n    \"\"\"\n    if not is_reasoning_model(model):\n        return messages\n\n    # Convert system messages to user messages for reasoning models\n    converted = []\n    for msg in messages:\n        if msg['role'] == 'system':\n            # Convert system message to user message with clear labeling\n            converted.append({\n                'role': 'user',\n                'content': f\"[System Context]\\n{msg['content']}\"\n            })\n        else:\n            converted.append(msg.copy())\n\n    # Merge consecutive user messages (reasoning models may require alternating roles)\n    merged = []\n    for msg in converted:\n        if merged and merged[-1]['role'] == msg['role'] == 'user':\n            merged[-1]['content'] += '\\n\\n' + msg['content']\n        else:\n            merged.append(msg)\n\n    return merged\n\n\ndef get_reasoning_model_params(model: str) -> Dict[str, Any]:\n    \"\"\"\n    Get reasoning model-specific parameters for the API request.\n\n    Returns parameters dict to merge with the payload for reasoning models.\n    \"\"\"\n    params = {}\n\n    if is_reasoning_model(model):\n        # Reasoning models don't support temperature\n        # Some may support reasoning_effort\n        params['reasoning_effort'] = 'high'\n\n    return params\n\n\nasync def query_model(\n    model: str,\n    messages: List[Dict[str, str]],\n    timeout: float = 120.0,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None\n) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Query a single model via OpenRouter API.\n\n    Args:\n        model: OpenRouter model identifier (e.g., \"openai/gpt-4o\")\n        messages: List of message dicts with 'role' and 'content'\n        timeout: Request timeout in seconds\n        temperature: Optional temperature for response generation (0.0-2.0)\n        max_tokens: Optional maximum tokens in response\n\n    Returns:\n        Response dict with 'content', optional 'reasoning_details', and metrics, or None if failed\n    \"\"\"\n    headers = {\n        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n        \"Content-Type\": \"application/json\",\n    }\n\n    # Prepare messages for this specific model (handles reasoning models)\n    prepared_messages = prepare_messages_for_model(model, messages)\n\n    # Build payload with model-specific parameters\n    payload = {\n        \"model\": model,\n        \"messages\": prepared_messages,\n    }","numLines":200,"startLine":1,"totalLines":551}}}
{"parentUuid":"7176d923-acb6-476f-9ff9-61a4ff925f2b","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01BivJ2ci86CNuZhiukbMdpc","type":"message","role":"assistant","content":[{"type":"text","text":"Let me continue reading openrouter.py:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2676,"cache_read_input_tokens":92889,"cache_creation":{"ephemeral_5m_input_tokens":2676,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWhmQ911mxu4iE2cFZgsf","type":"assistant","uuid":"5e6f6544-a598-4821-af84-8b8796c03b4f","timestamp":"2026-01-02T02:31:21.467Z"}
{"parentUuid":"5e6f6544-a598-4821-af84-8b8796c03b4f","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01BivJ2ci86CNuZhiukbMdpc","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01AhGm56pTpCwoSPNGWYJaXM","name":"Read","input":{"file_path":"/Users/jeremy/Projects/LLMCouncil/backend/openrouter.py","offset":200}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2676,"cache_read_input_tokens":92889,"cache_creation":{"ephemeral_5m_input_tokens":2676,"ephemeral_1h_input_tokens":0},"output_tokens":101,"service_tier":"standard"}},"requestId":"req_011CWhmQ911mxu4iE2cFZgsf","type":"assistant","uuid":"f2935bb7-6b70-4e71-99c9-8edc21b7e4dd","timestamp":"2026-01-02T02:31:21.911Z"}
{"parentUuid":"f2935bb7-6b70-4e71-99c9-8edc21b7e4dd","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AhGm56pTpCwoSPNGWYJaXM","type":"tool_result","content":"   200→    }\n   201→\n   202→    # Add reasoning model-specific parameters (e.g., reasoning_effort for o1/o3)\n   203→    reasoning_params = get_reasoning_model_params(model)\n   204→\n   205→    # For reasoning models, don't add temperature (they don't support it)\n   206→    if not is_reasoning_model(model):\n   207→        if temperature is not None:\n   208→            payload[\"temperature\"] = temperature\n   209→\n   210→    # Add max_tokens if specified\n   211→    if max_tokens is not None:\n   212→        payload[\"max_tokens\"] = max_tokens\n   213→\n   214→    payload.update(reasoning_params)\n   215→\n   216→    is_reasoning = is_reasoning_model(model)\n   217→    start_time = time.time()\n   218→\n   219→    try:\n   220→        async with httpx.AsyncClient(timeout=timeout) as client:\n   221→            response = await client.post(\n   222→                OPENROUTER_API_URL,\n   223→                headers=headers,\n   224→                json=payload\n   225→            )\n   226→            response.raise_for_status()\n   227→\n   228→            elapsed_ms = int((time.time() - start_time) * 1000)\n   229→            data = response.json()\n   230→            message = data['choices'][0]['message']\n   231→\n   232→            # Extract usage data if available\n   233→            usage = data.get('usage', {})\n   234→\n   235→            # Build response with reasoning model support\n   236→            result = {\n   237→                'content': message.get('content'),\n   238→                'reasoning_details': message.get('reasoning_details'),\n   239→                'is_reasoning_model': is_reasoning,\n   240→                'metrics': {\n   241→                    'response_time_ms': elapsed_ms,\n   242→                    'input_tokens': usage.get('prompt_tokens'),\n   243→                    'output_tokens': usage.get('completion_tokens'),\n   244→                    'total_tokens': usage.get('total_tokens'),\n   245→                }\n   246→            }\n   247→\n   248→            # For reasoning models, also capture reasoning_content if available\n   249→            if is_reasoning:\n   250→                result['reasoning_content'] = message.get('reasoning_content')\n   251→                result['reasoning_tokens'] = usage.get('reasoning_tokens')\n   252→\n   253→            return result\n   254→\n   255→    except Exception as e:\n   256→        elapsed_ms = int((time.time() - start_time) * 1000)\n   257→        print(f\"Error querying model {model}: {e}\")\n   258→        return {\n   259→            'content': None,\n   260→            'error': str(e),\n   261→            'is_reasoning_model': is_reasoning,\n   262→            'metrics': {\n   263→                'response_time_ms': elapsed_ms,\n   264→                'error_type': type(e).__name__,\n   265→            }\n   266→        }\n   267→\n   268→\n   269→async def query_models_parallel(\n   270→    models: List[str],\n   271→    messages: List[Dict[str, str]],\n   272→    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n   273→) -> Dict[str, Optional[Dict[str, Any]]]:\n   274→    \"\"\"\n   275→    Query multiple models in parallel.\n   276→\n   277→    Args:\n   278→        models: List of OpenRouter model identifiers\n   279→        messages: List of message dicts to send to each model\n   280→        model_parameters: Optional dict mapping model_id to {temperature, max_tokens}\n   281→\n   282→    Returns:\n   283→        Dict mapping model identifier to response dict (or None if failed)\n   284→    \"\"\"\n   285→    import asyncio\n   286→\n   287→    # Create tasks for all models with their specific parameters\n   288→    tasks = []\n   289→    for model in models:\n   290→        params = {}\n   291→        if model_parameters and model in model_parameters:\n   292→            mp = model_parameters[model]\n   293→            if \"temperature\" in mp:\n   294→                params[\"temperature\"] = mp[\"temperature\"]\n   295→            if \"max_tokens\" in mp:\n   296→                params[\"max_tokens\"] = mp[\"max_tokens\"]\n   297→        tasks.append(query_model(model, messages, **params))\n   298→\n   299→    # Wait for all to complete\n   300→    responses = await asyncio.gather(*tasks)\n   301→\n   302→    # Map models to their responses\n   303→    return {model: response for model, response in zip(models, responses)}\n   304→\n   305→\n   306→async def stream_model(\n   307→    model: str,\n   308→    messages: List[Dict[str, str]],\n   309→    timeout: float = 120.0,\n   310→    temperature: Optional[float] = None,\n   311→    max_tokens: Optional[int] = None\n   312→) -> AsyncGenerator[Dict[str, Any], None]:\n   313→    \"\"\"\n   314→    Stream tokens from a single model via OpenRouter API.\n   315→\n   316→    Args:\n   317→        model: OpenRouter model identifier (e.g., \"openai/gpt-4o\")\n   318→        messages: List of message dicts with 'role' and 'content'\n   319→        timeout: Request timeout in seconds\n   320→        temperature: Optional temperature for response generation (0.0-2.0)\n   321→        max_tokens: Optional maximum tokens in response\n   322→\n   323→    Yields:\n   324→        Dict with 'type' (token, reasoning, done, error) and 'content'\n   325→    \"\"\"\n   326→    headers = {\n   327→        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n   328→        \"Content-Type\": \"application/json\",\n   329→    }\n   330→\n   331→    # Prepare messages for this specific model (handles reasoning models)\n   332→    prepared_messages = prepare_messages_for_model(model, messages)\n   333→\n   334→    payload = {\n   335→        \"model\": model,\n   336→        \"messages\": prepared_messages,\n   337→        \"stream\": True,\n   338→    }\n   339→\n   340→    # Add reasoning model-specific parameters\n   341→    reasoning_params = get_reasoning_model_params(model)\n   342→\n   343→    # For reasoning models, don't add temperature (they don't support it)\n   344→    if not is_reasoning_model(model):\n   345→        if temperature is not None:\n   346→            payload[\"temperature\"] = temperature\n   347→\n   348→    # Add max_tokens if specified\n   349→    if max_tokens is not None:\n   350→        payload[\"max_tokens\"] = max_tokens\n   351→\n   352→    payload.update(reasoning_params)\n   353→\n   354→    is_reasoning = is_reasoning_model(model)\n   355→\n   356→    try:\n   357→        async with httpx.AsyncClient(timeout=timeout) as client:\n   358→            async with client.stream(\n   359→                \"POST\",\n   360→                OPENROUTER_API_URL,\n   361→                headers=headers,\n   362→                json=payload\n   363→            ) as response:\n   364→                response.raise_for_status()\n   365→\n   366→                full_content = \"\"\n   367→                full_reasoning = \"\"\n   368→                async for line in response.aiter_lines():\n   369→                    if not line or not line.startswith(\"data: \"):\n   370→                        continue\n   371→\n   372→                    data = line[6:]  # Remove \"data: \" prefix\n   373→                    if data == \"[DONE]\":\n   374→                        result = {\n   375→                            \"type\": \"done\",\n   376→                            \"content\": full_content,\n   377→                            \"is_reasoning_model\": is_reasoning\n   378→                        }\n   379→                        if is_reasoning and full_reasoning:\n   380→                            result[\"reasoning_content\"] = full_reasoning\n   381→                        yield result\n   382→                        return\n   383→\n   384→                    try:\n   385→                        chunk = json.loads(data)\n   386→                        delta = chunk.get(\"choices\", [{}])[0].get(\"delta\", {})\n   387→\n   388→                        # Handle regular content\n   389→                        content = delta.get(\"content\", \"\")\n   390→                        if content:\n   391→                            full_content += content\n   392→                            yield {\"type\": \"token\", \"content\": content}\n   393→\n   394→                        # Handle reasoning content for reasoning models\n   395→                        if is_reasoning:\n   396→                            reasoning = delta.get(\"reasoning_content\", \"\")\n   397→                            if reasoning:\n   398→                                full_reasoning += reasoning\n   399→                                yield {\"type\": \"reasoning\", \"content\": reasoning}\n   400→                    except json.JSONDecodeError:\n   401→                        continue\n   402→\n   403→    except httpx.HTTPStatusError as e:\n   404→        yield {\"type\": \"error\", \"content\": f\"HTTP {e.response.status_code}: {str(e)}\"}\n   405→    except Exception as e:\n   406→        yield {\"type\": \"error\", \"content\": str(e)}\n   407→\n   408→\n   409→# Circuit breaker state\n   410→_circuit_breaker_state: Dict[str, Dict[str, Any]] = {}\n   411→\n   412→\n   413→class CircuitBreaker:\n   414→    \"\"\"\n   415→    Circuit breaker pattern for model API calls.\n   416→\n   417→    States:\n   418→    - CLOSED: Normal operation, requests pass through\n   419→    - OPEN: Failing, reject requests immediately\n   420→    - HALF_OPEN: Testing if service recovered\n   421→    \"\"\"\n   422→\n   423→    FAILURE_THRESHOLD = 3  # failures before opening circuit\n   424→    RECOVERY_TIMEOUT = 60  # seconds before trying again\n   425→    SUCCESS_THRESHOLD = 2  # successes to close circuit\n   426→\n   427→    @classmethod\n   428→    def get_state(cls, model: str) -> Dict[str, Any]:\n   429→        \"\"\"Get circuit state for a model.\"\"\"\n   430→        import time\n   431→        if model not in _circuit_breaker_state:\n   432→            _circuit_breaker_state[model] = {\n   433→                \"state\": \"CLOSED\",\n   434→                \"failures\": 0,\n   435→                \"successes\": 0,\n   436→                \"last_failure_time\": 0\n   437→            }\n   438→        return _circuit_breaker_state[model]\n   439→\n   440→    @classmethod\n   441→    def can_execute(cls, model: str) -> bool:\n   442→        \"\"\"Check if request can proceed.\"\"\"\n   443→        import time\n   444→        state = cls.get_state(model)\n   445→\n   446→        if state[\"state\"] == \"CLOSED\":\n   447→            return True\n   448→\n   449→        if state[\"state\"] == \"OPEN\":\n   450→            # Check if recovery timeout has passed\n   451→            if time.time() - state[\"last_failure_time\"] >= cls.RECOVERY_TIMEOUT:\n   452→                state[\"state\"] = \"HALF_OPEN\"\n   453→                state[\"successes\"] = 0\n   454→                return True\n   455→            return False\n   456→\n   457→        # HALF_OPEN: allow request to test recovery\n   458→        return True\n   459→\n   460→    @classmethod\n   461→    def record_success(cls, model: str):\n   462→        \"\"\"Record successful request.\"\"\"\n   463→        state = cls.get_state(model)\n   464→\n   465→        if state[\"state\"] == \"HALF_OPEN\":\n   466→            state[\"successes\"] += 1\n   467→            if state[\"successes\"] >= cls.SUCCESS_THRESHOLD:\n   468→                state[\"state\"] = \"CLOSED\"\n   469→                state[\"failures\"] = 0\n   470→                state[\"successes\"] = 0\n   471→        else:\n   472→            state[\"failures\"] = 0\n   473→\n   474→    @classmethod\n   475→    def record_failure(cls, model: str):\n   476→        \"\"\"Record failed request.\"\"\"\n   477→        import time\n   478→        state = cls.get_state(model)\n   479→\n   480→        state[\"failures\"] += 1\n   481→        state[\"last_failure_time\"] = time.time()\n   482→\n   483→        if state[\"state\"] == \"HALF_OPEN\":\n   484→            state[\"state\"] = \"OPEN\"\n   485→        elif state[\"failures\"] >= cls.FAILURE_THRESHOLD:\n   486→            state[\"state\"] = \"OPEN\"\n   487→\n   488→    @classmethod\n   489→    def get_status(cls, model: str) -> str:\n   490→        \"\"\"Get current circuit state.\"\"\"\n   491→        return cls.get_state(model)[\"state\"]\n   492→\n   493→\n   494→async def query_model_with_circuit_breaker(\n   495→    model: str,\n   496→    messages: List[Dict[str, str]],\n   497→    timeout: float = 120.0,\n   498→    temperature: Optional[float] = None,\n   499→    max_tokens: Optional[int] = None\n   500→) -> Optional[Dict[str, Any]]:\n   501→    \"\"\"\n   502→    Query a model with circuit breaker protection.\n   503→\n   504→    Returns None if circuit is open or request fails.\n   505→    \"\"\"\n   506→    if not CircuitBreaker.can_execute(model):\n   507→        print(f\"Circuit breaker OPEN for {model}, skipping request\")\n   508→        return {\n   509→            'content': None,\n   510→            'error': 'Circuit breaker OPEN',\n   511→            'metrics': {\n   512→                'response_time_ms': 0,\n   513→                'error_type': 'CircuitBreakerOpen',\n   514→            }\n   515→        }\n   516→\n   517→    result = await query_model(model, messages, timeout, temperature, max_tokens)\n   518→\n   519→    if result is None or result.get('content') is None:\n   520→        CircuitBreaker.record_failure(model)\n   521→    else:\n   522→        CircuitBreaker.record_success(model)\n   523→\n   524→    return result\n   525→\n   526→\n   527→async def query_models_parallel_with_circuit_breaker(\n   528→    models: List[str],\n   529→    messages: List[Dict[str, str]],\n   530→    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n   531→) -> Dict[str, Optional[Dict[str, Any]]]:\n   532→    \"\"\"\n   533→    Query multiple models in parallel with circuit breaker protection.\n   534→    \"\"\"\n   535→    import asyncio\n   536→\n   537→    tasks = []\n   538→    for model in models:\n   539→        params = {}\n   540→        if model_parameters and model in model_parameters:\n   541→            mp = model_parameters[model]\n   542→            if \"temperature\" in mp:\n   543→                params[\"temperature\"] = mp[\"temperature\"]\n   544→            if \"max_tokens\" in mp:\n   545→                params[\"max_tokens\"] = mp[\"max_tokens\"]\n   546→        tasks.append(query_model_with_circuit_breaker(model, messages, **params))\n   547→\n   548→    responses = await asyncio.gather(*tasks)\n   549→\n   550→    return {model: response for model, response in zip(models, responses)}\n   551→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"17d9c0e1-5237-4fbe-954a-f1f547f69139","timestamp":"2026-01-02T02:31:22.928Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/jeremy/Projects/LLMCouncil/backend/openrouter.py","content":"    }\n\n    # Add reasoning model-specific parameters (e.g., reasoning_effort for o1/o3)\n    reasoning_params = get_reasoning_model_params(model)\n\n    # For reasoning models, don't add temperature (they don't support it)\n    if not is_reasoning_model(model):\n        if temperature is not None:\n            payload[\"temperature\"] = temperature\n\n    # Add max_tokens if specified\n    if max_tokens is not None:\n        payload[\"max_tokens\"] = max_tokens\n\n    payload.update(reasoning_params)\n\n    is_reasoning = is_reasoning_model(model)\n    start_time = time.time()\n\n    try:\n        async with httpx.AsyncClient(timeout=timeout) as client:\n            response = await client.post(\n                OPENROUTER_API_URL,\n                headers=headers,\n                json=payload\n            )\n            response.raise_for_status()\n\n            elapsed_ms = int((time.time() - start_time) * 1000)\n            data = response.json()\n            message = data['choices'][0]['message']\n\n            # Extract usage data if available\n            usage = data.get('usage', {})\n\n            # Build response with reasoning model support\n            result = {\n                'content': message.get('content'),\n                'reasoning_details': message.get('reasoning_details'),\n                'is_reasoning_model': is_reasoning,\n                'metrics': {\n                    'response_time_ms': elapsed_ms,\n                    'input_tokens': usage.get('prompt_tokens'),\n                    'output_tokens': usage.get('completion_tokens'),\n                    'total_tokens': usage.get('total_tokens'),\n                }\n            }\n\n            # For reasoning models, also capture reasoning_content if available\n            if is_reasoning:\n                result['reasoning_content'] = message.get('reasoning_content')\n                result['reasoning_tokens'] = usage.get('reasoning_tokens')\n\n            return result\n\n    except Exception as e:\n        elapsed_ms = int((time.time() - start_time) * 1000)\n        print(f\"Error querying model {model}: {e}\")\n        return {\n            'content': None,\n            'error': str(e),\n            'is_reasoning_model': is_reasoning,\n            'metrics': {\n                'response_time_ms': elapsed_ms,\n                'error_type': type(e).__name__,\n            }\n        }\n\n\nasync def query_models_parallel(\n    models: List[str],\n    messages: List[Dict[str, str]],\n    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n) -> Dict[str, Optional[Dict[str, Any]]]:\n    \"\"\"\n    Query multiple models in parallel.\n\n    Args:\n        models: List of OpenRouter model identifiers\n        messages: List of message dicts to send to each model\n        model_parameters: Optional dict mapping model_id to {temperature, max_tokens}\n\n    Returns:\n        Dict mapping model identifier to response dict (or None if failed)\n    \"\"\"\n    import asyncio\n\n    # Create tasks for all models with their specific parameters\n    tasks = []\n    for model in models:\n        params = {}\n        if model_parameters and model in model_parameters:\n            mp = model_parameters[model]\n            if \"temperature\" in mp:\n                params[\"temperature\"] = mp[\"temperature\"]\n            if \"max_tokens\" in mp:\n                params[\"max_tokens\"] = mp[\"max_tokens\"]\n        tasks.append(query_model(model, messages, **params))\n\n    # Wait for all to complete\n    responses = await asyncio.gather(*tasks)\n\n    # Map models to their responses\n    return {model: response for model, response in zip(models, responses)}\n\n\nasync def stream_model(\n    model: str,\n    messages: List[Dict[str, str]],\n    timeout: float = 120.0,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None\n) -> AsyncGenerator[Dict[str, Any], None]:\n    \"\"\"\n    Stream tokens from a single model via OpenRouter API.\n\n    Args:\n        model: OpenRouter model identifier (e.g., \"openai/gpt-4o\")\n        messages: List of message dicts with 'role' and 'content'\n        timeout: Request timeout in seconds\n        temperature: Optional temperature for response generation (0.0-2.0)\n        max_tokens: Optional maximum tokens in response\n\n    Yields:\n        Dict with 'type' (token, reasoning, done, error) and 'content'\n    \"\"\"\n    headers = {\n        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n        \"Content-Type\": \"application/json\",\n    }\n\n    # Prepare messages for this specific model (handles reasoning models)\n    prepared_messages = prepare_messages_for_model(model, messages)\n\n    payload = {\n        \"model\": model,\n        \"messages\": prepared_messages,\n        \"stream\": True,\n    }\n\n    # Add reasoning model-specific parameters\n    reasoning_params = get_reasoning_model_params(model)\n\n    # For reasoning models, don't add temperature (they don't support it)\n    if not is_reasoning_model(model):\n        if temperature is not None:\n            payload[\"temperature\"] = temperature\n\n    # Add max_tokens if specified\n    if max_tokens is not None:\n        payload[\"max_tokens\"] = max_tokens\n\n    payload.update(reasoning_params)\n\n    is_reasoning = is_reasoning_model(model)\n\n    try:\n        async with httpx.AsyncClient(timeout=timeout) as client:\n            async with client.stream(\n                \"POST\",\n                OPENROUTER_API_URL,\n                headers=headers,\n                json=payload\n            ) as response:\n                response.raise_for_status()\n\n                full_content = \"\"\n                full_reasoning = \"\"\n                async for line in response.aiter_lines():\n                    if not line or not line.startswith(\"data: \"):\n                        continue\n\n                    data = line[6:]  # Remove \"data: \" prefix\n                    if data == \"[DONE]\":\n                        result = {\n                            \"type\": \"done\",\n                            \"content\": full_content,\n                            \"is_reasoning_model\": is_reasoning\n                        }\n                        if is_reasoning and full_reasoning:\n                            result[\"reasoning_content\"] = full_reasoning\n                        yield result\n                        return\n\n                    try:\n                        chunk = json.loads(data)\n                        delta = chunk.get(\"choices\", [{}])[0].get(\"delta\", {})\n\n                        # Handle regular content\n                        content = delta.get(\"content\", \"\")\n                        if content:\n                            full_content += content\n                            yield {\"type\": \"token\", \"content\": content}\n\n                        # Handle reasoning content for reasoning models\n                        if is_reasoning:\n                            reasoning = delta.get(\"reasoning_content\", \"\")\n                            if reasoning:\n                                full_reasoning += reasoning\n                                yield {\"type\": \"reasoning\", \"content\": reasoning}\n                    except json.JSONDecodeError:\n                        continue\n\n    except httpx.HTTPStatusError as e:\n        yield {\"type\": \"error\", \"content\": f\"HTTP {e.response.status_code}: {str(e)}\"}\n    except Exception as e:\n        yield {\"type\": \"error\", \"content\": str(e)}\n\n\n# Circuit breaker state\n_circuit_breaker_state: Dict[str, Dict[str, Any]] = {}\n\n\nclass CircuitBreaker:\n    \"\"\"\n    Circuit breaker pattern for model API calls.\n\n    States:\n    - CLOSED: Normal operation, requests pass through\n    - OPEN: Failing, reject requests immediately\n    - HALF_OPEN: Testing if service recovered\n    \"\"\"\n\n    FAILURE_THRESHOLD = 3  # failures before opening circuit\n    RECOVERY_TIMEOUT = 60  # seconds before trying again\n    SUCCESS_THRESHOLD = 2  # successes to close circuit\n\n    @classmethod\n    def get_state(cls, model: str) -> Dict[str, Any]:\n        \"\"\"Get circuit state for a model.\"\"\"\n        import time\n        if model not in _circuit_breaker_state:\n            _circuit_breaker_state[model] = {\n                \"state\": \"CLOSED\",\n                \"failures\": 0,\n                \"successes\": 0,\n                \"last_failure_time\": 0\n            }\n        return _circuit_breaker_state[model]\n\n    @classmethod\n    def can_execute(cls, model: str) -> bool:\n        \"\"\"Check if request can proceed.\"\"\"\n        import time\n        state = cls.get_state(model)\n\n        if state[\"state\"] == \"CLOSED\":\n            return True\n\n        if state[\"state\"] == \"OPEN\":\n            # Check if recovery timeout has passed\n            if time.time() - state[\"last_failure_time\"] >= cls.RECOVERY_TIMEOUT:\n                state[\"state\"] = \"HALF_OPEN\"\n                state[\"successes\"] = 0\n                return True\n            return False\n\n        # HALF_OPEN: allow request to test recovery\n        return True\n\n    @classmethod\n    def record_success(cls, model: str):\n        \"\"\"Record successful request.\"\"\"\n        state = cls.get_state(model)\n\n        if state[\"state\"] == \"HALF_OPEN\":\n            state[\"successes\"] += 1\n            if state[\"successes\"] >= cls.SUCCESS_THRESHOLD:\n                state[\"state\"] = \"CLOSED\"\n                state[\"failures\"] = 0\n                state[\"successes\"] = 0\n        else:\n            state[\"failures\"] = 0\n\n    @classmethod\n    def record_failure(cls, model: str):\n        \"\"\"Record failed request.\"\"\"\n        import time\n        state = cls.get_state(model)\n\n        state[\"failures\"] += 1\n        state[\"last_failure_time\"] = time.time()\n\n        if state[\"state\"] == \"HALF_OPEN\":\n            state[\"state\"] = \"OPEN\"\n        elif state[\"failures\"] >= cls.FAILURE_THRESHOLD:\n            state[\"state\"] = \"OPEN\"\n\n    @classmethod\n    def get_status(cls, model: str) -> str:\n        \"\"\"Get current circuit state.\"\"\"\n        return cls.get_state(model)[\"state\"]\n\n\nasync def query_model_with_circuit_breaker(\n    model: str,\n    messages: List[Dict[str, str]],\n    timeout: float = 120.0,\n    temperature: Optional[float] = None,\n    max_tokens: Optional[int] = None\n) -> Optional[Dict[str, Any]]:\n    \"\"\"\n    Query a model with circuit breaker protection.\n\n    Returns None if circuit is open or request fails.\n    \"\"\"\n    if not CircuitBreaker.can_execute(model):\n        print(f\"Circuit breaker OPEN for {model}, skipping request\")\n        return {\n            'content': None,\n            'error': 'Circuit breaker OPEN',\n            'metrics': {\n                'response_time_ms': 0,\n                'error_type': 'CircuitBreakerOpen',\n            }\n        }\n\n    result = await query_model(model, messages, timeout, temperature, max_tokens)\n\n    if result is None or result.get('content') is None:\n        CircuitBreaker.record_failure(model)\n    else:\n        CircuitBreaker.record_success(model)\n\n    return result\n\n\nasync def query_models_parallel_with_circuit_breaker(\n    models: List[str],\n    messages: List[Dict[str, str]],\n    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n) -> Dict[str, Optional[Dict[str, Any]]]:\n    \"\"\"\n    Query multiple models in parallel with circuit breaker protection.\n    \"\"\"\n    import asyncio\n\n    tasks = []\n    for model in models:\n        params = {}\n        if model_parameters and model in model_parameters:\n            mp = model_parameters[model]\n            if \"temperature\" in mp:\n                params[\"temperature\"] = mp[\"temperature\"]\n            if \"max_tokens\" in mp:\n                params[\"max_tokens\"] = mp[\"max_tokens\"]\n        tasks.append(query_model_with_circuit_breaker(model, messages, **params))\n\n    responses = await asyncio.gather(*tasks)\n\n    return {model: response for model, response in zip(models, responses)}\n","numLines":352,"startLine":200,"totalLines":551}}}
{"parentUuid":"17d9c0e1-5237-4fbe-954a-f1f547f69139","isSidechain":true,"userType":"external","cwd":"/Users/jeremy/Projects/LLMCouncil/frontend","sessionId":"4dcea542-84bc-4531-9c4c-b8bf5b29dbe3","version":"2.0.75","gitBranch":"main","agentId":"a7233f1","slug":"tingly-growing-lemon","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01UQm8TPDm3YHjYi2qWKfUBW","type":"message","role":"assistant","content":[{"type":"text","text":"Perfect. Now I have a comprehensive understanding. Let me create a detailed report on my findings:\n\n## Analysis Report: LLM Council Prompt Design & Generic Response Issues\n\nBased on my examination of the LLMCouncil codebase, I've identified several critical issues with prompt design that directly contribute to generic, boring, and useless responses.\n\n### **Critical Finding Summary**\n\nThe council system has **minimal to no system prompts or instructions** guiding model behavior. The prompts are functional but lack depth, creativity guidance, and specific quality expectations. This is the root cause of generic responses.\n\n---\n\n### **1. Stage 1: Individual Responses - Minimal Guidance**\n\n**File**: `backend/council.py`, lines 146-228\n\n**Current Implementation**:\n```python\nasync def stage1_collect_responses(user_query: str, ...):\n    messages = build_messages_with_history(user_query, conversation_history)\n    \n    # Add confidence prompt if enabled\n    if enable_confidence:\n        # Find the user message and append confidence instruction\n        for msg in messages:\n            if msg['role'] == 'user':\n                msg['content'] = msg['content'] + '''\n\nPlease end your response with a confidence score in this exact format:\n[CONFIDENCE: X.X]\nWhere X.X is a number between 0.0 (not confident) and 1.0 (very confident).'''\n```\n\n**Problem**: The user's raw query is sent directly to models with **NO system prompt**. There is:\n- No instruction to be thorough or detailed\n- No instruction to provide examples or citations\n- No instruction to consider multiple perspectives\n- No instruction to flag assumptions or limitations\n- No instruction for response structure or format\n\n**Impact**: Models follow their defaults (often safe, generic, bland). The system is essentially asking \"Answer this\" with no context about what \"good\" means.\n\n---\n\n### **2. Stage 2: Ranking Prompts - Generic Evaluation**\n\n**File**: `backend/council.py`, lines 231-291\n\n**Current Implementation**:\n```python\ndef build_ranking_prompt(user_query: str, responses_text: str, ranking_criteria: Optional[List[Dict[str, Any]]] = None) -> str:\n    criteria_text = \"\"\n    if ranking_criteria:\n        enabled_criteria = [c for c in ranking_criteria if c.get('enabled', True)]\n        if enabled_criteria:\n            criteria_list = \"\\n\".join([\n                f\"- **{c['name']}**: {c.get('description', '')}\"\n                for c in enabled_criteria\n            ])\n            criteria_text = f\"\"\"\nEvaluate each response based on these criteria:\n{criteria_list}\n\n\"\"\"\n\n    return f\"\"\"You are evaluating different responses to the following question:\n\nQuestion: {user_query}\n\nHere are the responses from different models (anonymized):\n\n{responses_text}\n\nYour task:\n1. First, evaluate each response individually.{criteria_text} For each response, explain what it does well and what it does poorly.\n2. Then, at the very end of your response, provide a final ranking.\n\nIMPORTANT: Your final ranking MUST be formatted EXACTLY as follows:\n- Start with the line \"FINAL RANKING:\" (all caps, with colon)\n- Then list the responses from best to worst as a numbered list\n- Each line should be: number, period, space, then ONLY the response label (e.g., \"1. Response A\")\n- Do not add any other text or explanations in the ranking section\n```\n\n**Problems**:\n1. **Generic criteria**: When no custom criteria provided, models have NO guidance on what makes a response good. They default to vague notions like \"accuracy\" and \"completeness\"\n2. **No quality definition**: What does \"well\" and \"poorly\" mean? No specificity\n3. **Format over substance**: The prompt focuses heavily on getting the ranking format correct rather than deep evaluation\n4. **No reasoning depth**: Models aren't asked to explain trade-offs, nuances, or limitations\n5. **Anonymous responses reduce context**: Models see \"Response A, B, C\" with no indication of which model produced it, so they can't consider model specialties\n\n**Impact**: Stage 2 becomes a mechanical ranking exercise rather than thoughtful peer review. Evaluations are surface-level.\n\n---\n\n### **3. Stage 3: Chairman Synthesis - Minimal Context Integration**\n\n**File**: `backend/council.py`, lines 376-407\n\n**Current Implementation**:\n```python\ndef build_chairman_prompt(user_query: str, stage1_results: List[Dict[str, Any]], stage2_results: List[Dict[str, Any]]) -> str:\n    stage1_text = \"\\n\\n\".join([\n        f\"Model: {result['model']}\\nResponse: {result['response']}\"\n        for result in stage1_results\n    ])\n\n    stage2_text = \"\\n\\n\".join([\n        f\"Model: {result['model']}\\nRanking: {result['ranking']}\"\n        for result in stage2_results\n    ])\n\n    return f\"\"\"You are the Chairman of an LLM Council. Multiple AI models have provided responses to a user's question, and then ranked each other's responses.\n\nOriginal Question: {user_query}\n\nSTAGE 1 - Individual Responses:\n{stage1_text}\n\nSTAGE 2 - Peer Rankings:\n{stage2_text}\n\nYour task as Chairman is to synthesize all of this information into a single, comprehensive, accurate answer to the user's original question. Consider:\n- The individual responses and their insights\n- The peer rankings and what they reveal about response quality\n- Any patterns of agreement or disagreement\n\nProvide a clear, well-reasoned final answer that represents the council's collective wisdom:\"\"\"\n```\n\n**Problems**:\n1. **Vague synthesis instructions**: \"synthesize\", \"comprehensive\", \"accurate\" are not defined\n2. **No explicit quality standards**: What makes a synthesis \"good\"?\n3. **No structure guidance**: Should it compare approaches? Highlight consensus? Flag disagreement?\n4. **No instruction to improve**: The chairman just summarizes rather than synthesizing something new and better\n5. **No instruction for limitations**: Should it mention open questions or areas of uncertainty?\n6. **No instruction for evidence**: Should it cite which models said what? Should it add new perspective?\n\n**Impact**: Stage 3 reads like a summary of Stages 1-2 rather than a synthesis that adds value. The chairman doesn't improve on the raw responses.\n\n---\n\n### **4. Missing System Prompts Entirely**\n\n**File**: `backend/openrouter.py`, lines 168-225 and `backend/council.py`\n\n**Finding**: The system has **NO system prompts at all**. The `query_model()` function accepts only:\n- User messages\n- Temperature\n- Max tokens\n\n**No system role is ever set** for Stage 1 or Stage 3 calls. This is huge:\n\n```python\npayload = {\n    \"model\": model,\n    \"messages\": prepared_messages,  # Only user messages!\n}\n```\n\n**Impact**: Each model operates with its default instructions, leading to predictable, safe, generic responses.\n\n---\n\n### **5. Configuration System Ignores Prompt Customization**\n\n**File**: `backend/config.py`, `backend/main.py` (lines 96-107)\n\n**Current config options**:\n```python\nclass CouncilConfigRequest(BaseModel):\n    council_models: Optional[List[str]] = None\n    chairman_model: Optional[str] = None\n    theme: Optional[str] = None\n    ranking_criteria: Optional[List[Dict[str, Any]]] = None  # Only criteria!\n    model_weights: Optional[Dict[str, float]] = None\n    enable_confidence: Optional[bool] = None\n    enable_dissent_tracking: Optional[bool] = None\n    model_parameters: Optional[Dict[str, Dict[str, Any]]] = None\n```\n\n**Missing**:\n- No ability to customize system prompts\n- No ability to set response style preferences\n- No ability to specify desired output format\n- No ability to emphasize depth, creativity, or examples\n- No stage-specific instructions\n\n---\n\n### **6. Confidence Scoring Doesn't Improve Quality**\n\n**File**: `backend/council.py`, lines 91-143\n\nThe confidence scoring system just asks models to append a confidence number. This doesn't actually improve response quality—it just adds metadata. Models still don't know what makes a response high-quality.\n\n---\n\n### **Root Causes of Generic Responses**\n\n1. **No system prompts** - Models use their defaults\n2. **Vague task descriptions** - \"evaluate\", \"synthesize\", \"comprehensive\" without definition\n3. **No quality rubric** - No explicit standards for what constitutes a good response\n4. **No creativity prompts** - No instruction to provide novel insights or examples\n5. **No style guidance** - No specification for depth, detail level, or perspective\n6. **Mechanical focus** - Stage 2 focuses on ranking format rather than evaluation quality\n7. **Stage 3 doesn't add value** - Chairman just summarizes rather than synthesizing something better\n8. **No structure requests** - Responses can be unorganized or scattered\n9. **No explicit limitation flagging** - Models don't explain gaps or uncertainties\n10. **Anonymous ranking loses model context** - Stage 2 can't leverage model specialties\n\n---\n\n### **Specific Examples of Missing Instructions**\n\nA high-quality Stage 1 system prompt should include:\n- \"Provide at least one concrete example to illustrate your point\"\n- \"Explicitly state any assumptions you're making\"\n- \"Flag any limitations or gaps in your answer\"\n- \"Consider counterarguments to your position\"\n- \"Provide sources or evidence for key claims\"\n- \"Structure your answer with clear headings and sections\"\n- \"Aim for depth and nuance, not brevity\"\n\nA high-quality Stage 2 prompt should include:\n- \"Consider which response best addresses the root of the question\"\n- \"Flag which response acknowledges limitations or uncertainties\"\n- \"Identify which response provides the most specific, actionable advice\"\n- \"Consider which response would be most helpful to someone completely unfamiliar with this topic\"\n- \"Rate responses on: depth, accuracy, usefulness, novelty, clarity\"\n\nA high-quality Stage 3 prompt should include:\n- \"If responses disagree, explain why and synthesize a better approach\"\n- \"Identify what each response got right that others missed\"\n- \"Build on the best insights from all responses\"\n- \"Provide your own synthesis that improves on any single response\"\n- \"Flag remaining uncertainties or open questions\"\n\n---\n\n### **Summary**\n\nThe LLM Council system is architecturally sound but **suffers from prompt engineering poverty**. It's like hiring expert advisors but giving them no job description. The result is predictable, generic, risk-averse responses that don't leverage the potential of multiple models working together.\n\n**The fix requires**:\n1. Adding system prompts with explicit quality standards\n2. Creating detailed, domain-specific evaluation rubrics for Stage 2\n3. Instructing the chairman to synthesize rather than summarize\n4. Adding configurable prompt templates for different use cases\n5. Specifying response structure, depth, and style preferences\n6. Explicitly requesting examples, citations, and limitation acknowledgment"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":4347,"cache_read_input_tokens":95565,"cache_creation":{"ephemeral_5m_input_tokens":4347,"ephemeral_1h_input_tokens":0},"output_tokens":2653,"service_tier":"standard"}},"requestId":"req_011CWhmQmef3T9MiWRcUsV8P","type":"assistant","uuid":"40d5caa0-6d6a-4511-b160-30f9d73804e9","timestamp":"2026-01-02T02:31:52.660Z"}
